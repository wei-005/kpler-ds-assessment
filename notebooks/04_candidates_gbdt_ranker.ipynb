{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88e5afb",
   "metadata": {},
   "source": [
    "# 04 — Candidate Recall + GBDT Ranker\n",
    "# 04 — 候选召回 + GBDT 排序器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8038614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 项目根路径\n",
    "#Project root\n",
    "import sys\n",
    "from pathlib import Path\n",
    "proj_root = Path.cwd()\n",
    "if (proj_root.name.lower() == \"notebooks\" or (proj_root/\"utils\").exists() is False) and (proj_root.parent/\"utils\").exists():\n",
    "    proj_root = proj_root.parent\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.append(str(proj_root))\n",
    "print(\"Project root:\", proj_root)\n",
    "\n",
    "# 统一导入\n",
    "#Unified imports\n",
    "import numpy as np, pandas as pd, polars as pl, json, joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from utils.config import DATA_DIR, INTERIM_DIR, PROCESSED_DIR\n",
    "from utils.etl_clean import ensure_interim\n",
    "from utils.splits import temporal_split, add_crisis_flag\n",
    "from utils.candidates import build_origin_next_transitions, global_mf_next, build_pc_coords, build_candidates_for_split\n",
    "from utils.features import build_ports_attr, compute_port_degree, attach_port_side, build_sample_side, merge_all_features\n",
    "from utils.metrics import eval_topk_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5135281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入样本与清洗表\n",
    "samples = pl.read_parquet(PROCESSED_DIR / \"samples_taskA.parquet\")\n",
    "pc = pl.read_parquet(INTERIM_DIR / \"port_calls.cleaned.parquet\")\n",
    "tr = pl.read_csv(DATA_DIR / \"trades.csv\",  try_parse_dates=True)\n",
    "vs = pl.read_csv(DATA_DIR / \"vessels.csv\", try_parse_dates=True)\n",
    "\n",
    "train, val, test = temporal_split(samples)\n",
    "train = add_crisis_flag(train); val = add_crisis_flag(val); test = add_crisis_flag(test)\n",
    "\n",
    "trans = build_origin_next_transitions(train)\n",
    "g_top = global_mf_next(trans)\n",
    "pc_coords = build_pc_coords(pc)\n",
    "\n",
    "cand_train = build_candidates_for_split(train, trans, pc_coords, add_true_label=True,  N=10, M=10, global_top1=g_top)\n",
    "cand_val   = build_candidates_for_split(val,   trans, pc_coords, add_true_label=True,  N=10, M=10, global_top1=g_top)\n",
    "cand_test  = build_candidates_for_split(test,  trans, pc_coords, add_true_label=False, N=10, M=10, global_top1=g_top)\n",
    "\n",
    "ports_attr  = build_ports_attr(pc_coords)\n",
    "port_degree = compute_port_degree(trans)\n",
    "cand_train  = attach_port_side(cand_train, ports_attr, port_degree)\n",
    "cand_val    = attach_port_side(cand_val,   ports_attr, port_degree)\n",
    "cand_test   = attach_port_side(cand_test,  ports_attr, port_degree)\n",
    "\n",
    "s_side   = build_sample_side(samples, pc, vs)\n",
    "cand_train = merge_all_features(cand_train, s_side, train)\n",
    "cand_val   = merge_all_features(cand_val,   s_side, val)\n",
    "cand_test  = merge_all_features(cand_test,  s_side, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c9adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练 GBDT 排序器（HGB）\n",
    "num_cols = [\"dist_km\",\"is_same_region\",\"in_cnt\",\"out_cnt\",\"age\",\n",
    "            \"prev_dist_km\",\"last_leg_knots_est\",\"month_sin\",\"month_cos\",\"dow_sin\",\"dow_cos\",\n",
    "            \"is_crisis_time\",\"dist_x_crisis\"]\n",
    "cat_cols = [\"origin\",\"candidate\",\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"]\n",
    "\n",
    "def to_xy(df: pl.DataFrame):\n",
    "    keep = [\"sample_port_call_id\",\"origin\",\"candidate\",\"label\",\"y\"] + num_cols + cat_cols\n",
    "    for c in num_cols:\n",
    "        if c not in df.columns: df = df.with_columns(pl.lit(0.0).alias(c))\n",
    "    for c in cat_cols:\n",
    "        if c not in df.columns: df = df.with_columns(pl.lit(\"unk\").alias(c))\n",
    "    pdf = df.select(keep).to_pandas()\n",
    "    X_num = pdf[num_cols].values\n",
    "    # 将类别独热编码为 dense（HGB 不吃稀疏）\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    X_cat = enc.fit_transform(pdf[cat_cols])\n",
    "    X = np.hstack([X_num, X_cat])\n",
    "    y = pdf[\"y\"].values\n",
    "    meta = pdf[[\"sample_port_call_id\",\"origin\",\"candidate\",\"label\"]]\n",
    "    return X, y, meta, enc\n",
    "\n",
    "Xtr, ytr, mtr, enc = to_xy(cand_train)\n",
    "# 确保 val/test 使用同一个 encoder\n",
    "def to_xy_with_enc(df: pl.DataFrame, enc):\n",
    "    keep = [\"sample_port_call_id\",\"origin\",\"candidate\",\"label\",\"y\"] + num_cols + cat_cols\n",
    "    for c in num_cols:\n",
    "        if c not in df.columns: df = df.with_columns(pl.lit(0.0).alias(c))\n",
    "    for c in cat_cols:\n",
    "        if c not in df.columns: df = df.with_columns(pl.lit(\"unk\").alias(c))\n",
    "    pdf = df.select(keep).to_pandas()\n",
    "    X_num = pdf[num_cols].values\n",
    "    X_cat = enc.transform(pdf[cat_cols])\n",
    "    X = np.hstack([X_num, X_cat])\n",
    "    y = pdf[\"y\"].values\n",
    "    meta = pdf[[\"sample_port_call_id\",\"origin\",\"candidate\",\"label\"]]\n",
    "    return X, y, meta\n",
    "\n",
    "Xva, yva, mva = to_xy_with_enc(cand_val, enc)\n",
    "Xte, yte, mte = to_xy_with_enc(cand_test, enc)\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_depth=8, learning_rate=0.08, max_iter=300)\n",
    "clf.fit(Xtr, ytr)\n",
    "\n",
    "def rank_predict_hgb(clf, X, meta, ks=(1,3,5)):\n",
    "    proba = clf.predict_proba(X)[:,1]\n",
    "    meta2 = meta.copy()\n",
    "    meta2[\"score\"] = proba\n",
    "    topk = {}\n",
    "    for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "        g2 = g.sort_values(\"score\", ascending=False)\n",
    "        topk[sid] = g2[\"candidate\"].tolist()\n",
    "    truth, preds = [], []\n",
    "    for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "        truth.append(g[\"label\"].iloc[0])\n",
    "        preds.append(topk[sid])\n",
    "    return preds, truth\n",
    "\n",
    "from utils.metrics import eval_topk_mrr\n",
    "preds_val, truth_val = rank_predict_hgb(clf, Xva, mva)\n",
    "preds_te,  truth_te  = rank_predict_hgb(clf, Xte, mte)\n",
    "print(\"VAL:\", eval_topk_mrr([p[:5] for p in preds_val], truth_val, ks=(1,3,5)))\n",
    "print(\"TEST:\", eval_topk_mrr([p[:5] for p in preds_te],  truth_te,  ks=(1,3,5)))\n",
    "\n",
    "# 保存模型和 encoder\n",
    "import joblib\n",
    "joblib.dump({\"clf\":clf, \"enc\":enc, \"num_cols\":num_cols, \"cat_cols\":cat_cols}, PROCESSED_DIR / \"model_taskA_gbdt.joblib\")\n",
    "print(\"saved:\", PROCESSED_DIR / \"model_taskA_gbdt.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpler-ds (Py3.11)",
   "language": "python",
   "name": "kpler-ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
