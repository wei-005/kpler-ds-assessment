{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4f84de",
   "metadata": {},
   "source": [
    "\n",
    "# 04 · GBDT Ranker for Task‑A (self-contained)\n",
    "\n",
    "This notebook trains a Gradient Boosted Decision Tree **ranker** to predict a vessel's **very next destination** (Task‑A).\n",
    "It **does not rely on `taskA_candidate_pairs.parquet` generated by 03**. Instead, it rebuilds the candidate pairs and features (with caching) and then trains:\n",
    "\n",
    "- **Preferred**: `LightGBMRanker` (`objective=\"lambdarank\"`) if LightGBM is available.\n",
    "- **Fallback**: `HistGradientBoostingClassifier` + one-hot encoding if LightGBM is not available.\n",
    "\n",
    "The pipeline:\n",
    "1. Load `samples_taskA.parquet` (or rebuild from `port_calls.cleaned.parquet` if missing).\n",
    "2. Temporal split: Train / Valid / Test (via `utils.splits.temporal_split`).\n",
    "3. Build candidates per sample (Top‑N historical + Geo‑M + GlobalTop1; cached).\n",
    "4. Attach port-side features (coords/regions/degrees) and sample-side features (vessel attrs, seasonality, last leg speed, etc.).\n",
    "5. **Group-aware downsampling** on train (keep all positives; sample negatives per group).\n",
    "6. Train & evaluate the ranker; export metrics and top‑K predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a38a1ff-911e-4291-b29b-fb0305ceaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjustable: Add parent directory (which contains utils/) to Python search path\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  #  notebooks  sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cac66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR     = /Users/wangwei/Documents/Folders/工作/Kpler/data/raw\n",
      "INTERIM_DIR  = /Users/wangwei/Documents/Folders/工作/Kpler/data/interim\n",
      "PROCESSED_DIR= /Users/wangwei/Documents/Folders/工作/Kpler/data/processed\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np, pandas as pd, json, joblib, warnings\n",
    "\n",
    "# Fallback — Logistic with sparse OHE to reduce memory\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Use all cores for Polars\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = str(os.cpu_count())\n",
    "\n",
    "# Display setup\n",
    "pl.Config.set_tbl_rows(5)\n",
    "pl.Config.set_tbl_cols(12)\n",
    "\n",
    "# String cache for joins (compat across Polars versions)\n",
    "if hasattr(pl, \"enable_string_cache\"):\n",
    "    pl.enable_string_cache()   # Polars ≥1.0\n",
    "elif hasattr(pl, \"toggle_string_cache\"):\n",
    "    pl.toggle_string_cache(True)\n",
    "\n",
    "# Utils\n",
    "from utils.config import DATA_DIR, INTERIM_DIR, PROCESSED_DIR\n",
    "from utils.etl_clean import ensure_interim\n",
    "from utils.splits import temporal_split, add_crisis_flag\n",
    "from utils.candidates import (\n",
    "    build_origin_next_transitions, global_mf_next, build_pc_coords,\n",
    "    build_conditional_transitions, build_candidates_for_split, global_top_list\n",
    ")\n",
    "from utils.features import build_ports_attr, compute_port_degree, attach_port_side, build_sample_side, merge_all_features\n",
    "from utils.metrics import eval_topk_mrr\n",
    "\n",
    "print(\"DATA_DIR     =\", DATA_DIR)\n",
    "print(\"INTERIM_DIR  =\", INTERIM_DIR)\n",
    "print(\"PROCESSED_DIR=\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62897cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 260606  Val rows: 56201  Test rows: 21553\n"
     ]
    }
   ],
   "source": [
    "# Load samples (require precomputed by 01/03)\n",
    "samples_path = PROCESSED_DIR / \"samples_taskA.parquet\"\n",
    "assert samples_path.exists(), f\"Missing {samples_path}. Please run notebooks 01/03 first or set KPLER_PROCESSED_DIR.\"\n",
    "samples = pl.read_parquet(samples_path)\n",
    "\n",
    "# Ensure temporal dtype\n",
    "if samples.schema.get(\"call_ts\") == pl.Utf8:\n",
    "    samples = samples.with_columns(pl.col(\"call_ts\").str.strptime(pl.Datetime, strict=False))\n",
    "\n",
    "# Temporal split\n",
    "train, val, test = temporal_split(samples)\n",
    "train = add_crisis_flag(train); val = add_crisis_flag(val); test = add_crisis_flag(test)\n",
    "\n",
    "print(\"Train rows:\", len(train), \" Val rows:\", len(val), \" Test rows:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1026549b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cand_train: (13038518, 9) cand_val: (3365558, 9) cand_test: (1442035, 9)\n"
     ]
    }
   ],
   "source": [
    "# Build sample-side features early for conditional transitions\n",
    "cleaned = INTERIM_DIR / \"port_calls.cleaned.parquet\"\n",
    "if not cleaned.exists():\n",
    "    _ = ensure_interim()\n",
    "pc = pl.read_parquet(cleaned)\n",
    "vs = pl.read_csv(DATA_DIR / \"vessels.csv\", try_parse_dates=True) if (DATA_DIR / \"vessels.csv\").exists() else pl.DataFrame({})\n",
    "s_side   = build_sample_side(samples, pc, vs)\n",
    "\n",
    "# Build transitions and coords\n",
    "cleaned = INTERIM_DIR / \"port_calls.cleaned.parquet\"\n",
    "trans = build_origin_next_transitions(train)\n",
    "g_top  = global_mf_next(trans)\n",
    "# g_top3 = global_top_list(trans, K=3)\n",
    "g_top5 = global_top_list(trans, K=5)\n",
    "\n",
    "# Prefer cleaned parquet; if missing and raw also missing, derive coords from cached candidates\n",
    "pc_coords = None\n",
    "if cleaned.exists():\n",
    "    pc = pl.read_parquet(cleaned)\n",
    "    pc_coords = build_pc_coords(pc)\n",
    "else:\n",
    "    # Try to synthesize coordinates from cached candidates if available\n",
    "    cache_train = PROCESSED_DIR / \"cand_train_cached.parquet\"\n",
    "    cache_val   = PROCESSED_DIR / \"cand_val_cached.parquet\"\n",
    "    cc = []\n",
    "    if cache_train.exists(): cc.append(pl.read_parquet(cache_train))\n",
    "    if cache_val.exists():   cc.append(pl.read_parquet(cache_val))\n",
    "    if cc:\n",
    "        cc = pl.concat(cc) if len(cc)>1 else cc[0]\n",
    "        origin_coords = (cc.select([\"origin\",\"lat_o\",\"lon_o\"]).rename({\"origin\":\"destination\",\"lat_o\":\"lat\",\"lon_o\":\"lon\"}))\n",
    "        cand_coords   = (cc.select([\"candidate\",\"lat_c\",\"lon_c\"]).rename({\"candidate\":\"destination\",\"lat_c\":\"lat\",\"lon_c\":\"lon\"}))\n",
    "        pc_coords = pl.concat([origin_coords, cand_coords]).drop_nulls().unique()\n",
    "    else:\n",
    "        # As last resort, rebuild cleaned parquet (requires raw files present)\n",
    "        _ = ensure_interim()\n",
    "        pc = pl.read_parquet(cleaned)\n",
    "        pc_coords = build_pc_coords(pc)\n",
    "\n",
    "# Candidate caches + Conditional transitions (by vessel_type+dwt_bucket)\n",
    "cache_train = PROCESSED_DIR / \"cand_train_cached.parquet\"\n",
    "cache_val   = PROCESSED_DIR / \"cand_val_cached.parquet\"\n",
    "cache_test  = PROCESSED_DIR / \"cand_test_cached.parquet\"\n",
    "FORCE_REBUILD = True  # set True to ignore cache and regenerate\n",
    "\n",
    "cond_cols = [c for c in [\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"] if c in s_side.columns or c in samples.columns]\n",
    "# Enrich splits with conditional columns\n",
    "def enrich_with_conditions(split_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    add_cols = [\"sample_port_call_id\"] + [c for c in cond_cols if c in s_side.columns]\n",
    "    out = split_df.join(s_side.select(add_cols), on=\"sample_port_call_id\", how=\"left\")\n",
    "    # product_family_dom may already be in samples; ensure present\n",
    "    if \"product_family_dom\" in samples.columns and \"product_family_dom\" not in out.columns:\n",
    "        out = out.join(samples.select([\"sample_port_call_id\",\"product_family_dom\"]), on=\"sample_port_call_id\", how=\"left\")\n",
    "    return out\n",
    "train_en = enrich_with_conditions(train)\n",
    "val_en   = enrich_with_conditions(val)\n",
    "test_en  = enrich_with_conditions(test)\n",
    "\n",
    "# Build conditional transitions on training split\n",
    "trans_cond = build_conditional_transitions(train_en, condition_cols=[c for c in [\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"] if c in train_en.columns], top_k=10)\n",
    "\n",
    "if cache_train.exists() and not FORCE_REBUILD:\n",
    "    cand_train = pl.read_parquet(cache_train)\n",
    "else:\n",
    "    cand_train = build_candidates_for_split(train_en, trans, pc_coords, add_true_label=True, N=25, M=30, global_top1=g_top, global_top_list=g_top5, trans_cond=trans_cond, condition_cols=[c for c in [\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"] if c in train_en.columns], M_stages=[15,30], max_cands_per_sample=60)\n",
    "    cand_train.write_parquet(cache_train)\n",
    "\n",
    "if cache_val.exists() and not FORCE_REBUILD:\n",
    "    cand_val = pl.read_parquet(cache_val)\n",
    "else:\n",
    "    cand_val = build_candidates_for_split(val_en,   trans, pc_coords, add_true_label=True, N=25, M=30, global_top1=g_top, global_top_list=g_top5, trans_cond=trans_cond, condition_cols=[c for c in [\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"] if c in val_en.columns], M_stages=[20,35,50], max_cands_per_sample=60)\n",
    "    cand_val.write_parquet(cache_val)\n",
    "\n",
    "if cache_test.exists() and not FORCE_REBUILD:\n",
    "    cand_test = pl.read_parquet(cache_test)\n",
    "else:\n",
    "    cand_test = build_candidates_for_split(test_en,  trans, pc_coords, add_true_label=False, N=25, M=30, global_top1=g_top, global_top_list=g_top5, trans_cond=trans_cond, condition_cols=[c for c in [\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"] if c in test_en.columns], M_stages=[20,35,50], max_cands_per_sample=80)\n",
    "    cand_test.write_parquet(cache_test)\n",
    "\n",
    "def assert_no_dup_names(df: pl.DataFrame, name: str):\n",
    "    cols = df.columns\n",
    "    dups = [c for c in set(cols) if cols.count(c) > 1]\n",
    "    assert not dups, f\"{name} has duplicate columns: {dups}\"\n",
    "\n",
    "assert_no_dup_names(cand_train, \"cand_train\")\n",
    "assert_no_dup_names(cand_val,   \"cand_val\")\n",
    "assert_no_dup_names(cand_test,  \"cand_test\")\n",
    "\n",
    "print(\"cand_train:\", cand_train.shape, \"cand_val:\", cand_val.shape, \"cand_test:\", cand_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2076f42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL candidate recall:  100.00%\n",
      "TEST candidate recall: 87.38%\n",
      "Saved candidate pairs to: /Users/wangwei/Documents/Folders/工作/Kpler/data/processed/taskA_candidate_pairs.parquet\n"
     ]
    }
   ],
   "source": [
    "# Port-side attributes & degrees\n",
    "ports_attr  = build_ports_attr(pc_coords)\n",
    "port_degree = compute_port_degree(trans)\n",
    "\n",
    "# Helper: candidate recall\n",
    "def candidate_recall(df: pl.DataFrame) -> float:\n",
    "    return float(df.group_by(\"sample_port_call_id\").agg(pl.col(\"y\").max()).select(pl.col(\"y\").mean()).item())\n",
    "\n",
    "# Sample-side features (vessel attrs, seasonal, last leg speed, etc.)\n",
    "vs = pl.read_csv(DATA_DIR / \"vessels.csv\", try_parse_dates=True) if (DATA_DIR / \"vessels.csv\").exists() else pl.DataFrame({})\n",
    "s_side = build_sample_side(samples, pc, vs)\n",
    "\n",
    "# --- Train ---\n",
    "cand_train  = attach_port_side(cand_train, ports_attr, port_degree, compute_distance=False)\n",
    "cand_train  = merge_all_features(cand_train, s_side, train)\n",
    "assert_no_dup_names(cand_train, \"cand_train (post-merge)\")\n",
    "train_out = PROCESSED_DIR / \"cand_train_enriched.parquet\"\n",
    "cand_train.write_parquet(train_out)\n",
    "del cand_train; import gc; gc.collect()\n",
    "\n",
    "# --- Val ---\n",
    "cand_val    = attach_port_side(cand_val, ports_attr, port_degree, compute_distance=False)\n",
    "cand_val    = merge_all_features(cand_val, s_side, val)\n",
    "assert_no_dup_names(cand_val,   \"cand_val (post-merge)\")\n",
    "val_recall = candidate_recall(cand_val)\n",
    "print(f\"VAL candidate recall:  {val_recall:.2%}\")\n",
    "val_out = PROCESSED_DIR / \"cand_val_enriched.parquet\"\n",
    "cand_val.write_parquet(val_out)\n",
    "del cand_val; import gc; gc.collect()\n",
    "\n",
    "# --- Test ---\n",
    "cand_test   = attach_port_side(cand_test, ports_attr, port_degree, compute_distance=False)\n",
    "cand_test   = merge_all_features(cand_test, s_side, test)\n",
    "assert_no_dup_names(cand_test,  \"cand_test (post-merge)\")\n",
    "test_recall = candidate_recall(cand_test)\n",
    "print(f\"TEST candidate recall: {test_recall:.2%}\")\n",
    "test_out = PROCESSED_DIR / \"cand_test_enriched.parquet\"\n",
    "cand_test.write_parquet(test_out)\n",
    "del cand_test; import gc; gc.collect()\n",
    "\n",
    "# Combine pairs at the end (read from enriched files)\n",
    "pairs_out = PROCESSED_DIR / \"taskA_candidate_pairs.parquet\"\n",
    "cand_train_en = pl.read_parquet(train_out).with_columns(pl.lit(\"train\").alias(\"split\"))\n",
    "cand_val_en   = pl.read_parquet(val_out).with_columns(pl.lit(\"valid\").alias(\"split\"))\n",
    "cand_test_en  = pl.read_parquet(test_out).with_columns(pl.lit(\"test\").alias(\"split\"))\n",
    "pl.concat([cand_train_en, cand_val_en, cand_test_en]).write_parquet(pairs_out)\n",
    "print(\"Saved candidate pairs to:\", pairs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74d2f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (unique): (700000, 21)  -> Downsampled: (700000, 21)\n",
      "Val   (unique): (100000, 21)\n",
      "Test  (unique): (100000, 21)\n",
      "LightGBM disabled for this run; using logistic fallback.\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Total groups: 243349, total data: 700000\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4236\n",
      "[LightGBM] [Info] Number of data points in the train set: 700000, number of used features: 15\n",
      "[LightGBM] [Info] Total groups: 46898, total data: 100000\n",
      "[50]\tvalid_0's map@1: 0.991684\tvalid_0's map@3: 0.995252\tvalid_0's map@5: 0.995367\n",
      "[100]\tvalid_0's map@1: 0.991876\tvalid_0's map@3: 0.995345\tvalid_0's map@5: 0.995455\n",
      "[150]\tvalid_0's map@1: 0.99194\tvalid_0's map@3: 0.99541\tvalid_0's map@5: 0.995505\n",
      "[200]\tvalid_0's map@1: 0.99194\tvalid_0's map@3: 0.995369\tvalid_0's map@5: 0.995502\n",
      "[250]\tvalid_0's map@1: 0.991812\tvalid_0's map@3: 0.995295\tvalid_0's map@5: 0.995427\n",
      "VAL: {'hits@1': 0.025630090835430084, 'hits@3': 0.03358352168535972, 'hits@5': 0.03394601049085249, 'mrr': 0.029419520377556945}\n",
      "TEST: {'hits@1': 0.037217358737886806, 'hits@3': 0.0535087308646599, 'hits@5': 0.058330602499882964, 'mrr': 0.04552611457016691}\n",
      "Saved model: /Users/wangwei/Documents/Folders/工作/Kpler/data/processed/model_taskA_lgbm_ranker.txt\n",
      "Saved: /Users/wangwei/Documents/Folders/工作/Kpler/data/processed/rank_top5_val.parquet\n",
      "Saved: /Users/wangwei/Documents/Folders/工作/Kpler/data/processed/rank_top5_test.parquet\n",
      "CPU times: user 4min 23s, sys: 7min 7s, total: 11min 31s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Group-aware negative downsampling (keep all positives per sample)\n",
    "def downsample_groupwise(df: pl.DataFrame, neg_per_pos:int=15, seed:int=42) -> pl.DataFrame:\n",
    "    out = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    for sid, sub in df.group_by(\"sample_port_call_id\", maintain_order=True):\n",
    "        pos = sub.filter(pl.col(\"y\")==1)\n",
    "        neg = sub.filter(pl.col(\"y\")==0)\n",
    "        if len(pos) == 0:\n",
    "            k = min(20, len(neg))\n",
    "            if k > 0:\n",
    "                keep_idx = rng.choice(len(neg), size=k, replace=False)\n",
    "                out.append(neg[keep_idx])\n",
    "            continue\n",
    "        k = min(len(neg), neg_per_pos * len(pos))\n",
    "        if k > 0:\n",
    "            keep_idx = rng.choice(len(neg), size=k, replace=False)\n",
    "            out.append(pl.concat([pos, neg[keep_idx]]))\n",
    "        else:\n",
    "            out.append(pos)\n",
    "    return pl.concat(out) if out else df.head(0)\n",
    "\n",
    "train_keep_cols = [\"sample_port_call_id\",\"origin\",\"candidate\",\"label\",\"y\"]\n",
    "\n",
    "# Numeric features\n",
    "num_cols = [\n",
    "    \"dist_km\",\"is_same_region\",\"in_cnt\",\"out_cnt\",\"age\",\n",
    "    \"prev_dist_km\",\"last_leg_knots_est\",\n",
    "    \"month_sin\",\"month_cos\",\"dow_sin\",\"dow_cos\",\n",
    "    \"is_crisis_time\",\"dist_x_crisis\"\n",
    "]\n",
    "\n",
    "# Paths to enriched candidate parquet files (written in previous cell)\n",
    "train_enriched_path = PROCESSED_DIR / \"cand_train_enriched.parquet\"\n",
    "val_enriched_path   = PROCESSED_DIR / \"cand_val_enriched.parquet\"\n",
    "test_enriched_path  = PROCESSED_DIR / \"cand_test_enriched.parquet\"\n",
    "\n",
    "# extra numeric if available\n",
    "cand_cols = pl.read_parquet(val_enriched_path).columns if val_enriched_path.exists() else []\n",
    "extra_num = [c for c in [\n",
    "    \"prior_prob_oc\",\"hist_cnt_oc\",\"prior_prob_oc_vtype\",\"prior_prob_oc_dwt\",\n",
    "    \"prior_prob_oc_laden\",\"prior_prob_oc_pf\",\n",
    "    \"geo_rank_in_sample\",\"log_dist_km\",\"cand_is_waypoint\",\n",
    "    \"in_cnt_cand\",\"out_cnt_cand\"\n",
    "] if c in cand_cols]\n",
    "num_cols = num_cols + extra_num\n",
    "\n",
    "# Categorical features\n",
    "cat_cols = [c for c in [\"origin\",\"candidate\",\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"] if c in cand_cols]\n",
    "\n",
    "# Build unique keep cols and backfill missing columns across splits\n",
    "keep_cols = list(dict.fromkeys(train_keep_cols + num_cols + cat_cols))\n",
    "def ensure_cols(df: pl.DataFrame, cols):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    for c in missing:\n",
    "        df = df.with_columns((pl.lit(0.0) if c in num_cols else pl.lit(\"unk\")).alias(c))\n",
    "    return df\n",
    "\n",
    "def load_unique(path) -> pl.DataFrame:\n",
    "    df = pl.read_parquet(path)\n",
    "    df = ensure_cols(df, keep_cols)\n",
    "    return df.select(keep_cols).unique(subset=[\"sample_port_call_id\",\"candidate\"])\n",
    "\n",
    "cand_train_u = load_unique(train_enriched_path)\n",
    "cand_val_u   = load_unique(val_enriched_path)\n",
    "cand_test_u  = load_unique(test_enriched_path)\n",
    "\n",
    "# Memory guard: subsample validation/test for demonstration\n",
    "def subsample(df: pl.DataFrame, n:int=100_000, seed:int=42) -> pl.DataFrame:\n",
    "    return df if len(df) <= n else df.sample(n=n, seed=seed)\n",
    "cand_val_u  = subsample(cand_val_u,  n=100_000)\n",
    "cand_test_u = subsample(cand_test_u, n=100_000)\n",
    "\n",
    "# Optional pre-sample training set to control memory (similar to 03)\n",
    "if len(cand_train_u) > 700_000:\n",
    "    cand_train_u = cand_train_u.sample(n=700_000, seed=42)\n",
    "cand_train_ds = downsample_groupwise(cand_train_u, neg_per_pos=12, seed=42)\n",
    "\n",
    "print(\"Train (unique):\", cand_train_u.shape, \" -> Downsampled:\", cand_train_ds.shape)\n",
    "print(\"Val   (unique):\", cand_val_u.shape)\n",
    "print(\"Test  (unique):\", cand_test_u.shape)\n",
    "\n",
    "# Disable LightGBM in this run to avoid heavy memory/OMP issues; use logistic fallback\n",
    "use_lgbm = True\n",
    "print(\"LightGBM disabled for this run; using logistic fallback.\")\n",
    "\n",
    "def prep_for_lgb(df_pl: pl.DataFrame):\n",
    "    df_pd = df_pl.select(keep_cols).to_pandas()\n",
    "    for c in cat_cols:\n",
    "        df_pd[c] = df_pd[c].astype(\"category\")\n",
    "    for c in num_cols:\n",
    "        if c in df_pd.columns:\n",
    "            df_pd[c] = pd.to_numeric(df_pd[c], errors=\"coerce\").replace([np.inf, -np.inf], np.nan)\n",
    "    X = df_pd[num_cols + cat_cols]\n",
    "    y = df_pd[\"y\"].astype(int).values\n",
    "    group_sizes = df_pd.groupby(\"sample_port_call_id\")[\"candidate\"].size().values.tolist()\n",
    "    meta = df_pd[[\"sample_port_call_id\",\"origin\",\"candidate\",\"label\"]]\n",
    "    return X, y, group_sizes, meta\n",
    "\n",
    "if use_lgbm:\n",
    "    Xtr, ytr, gtr, mtr = prep_for_lgb(cand_train_ds)\n",
    "    Xva, yva, gva, mva = prep_for_lgb(cand_val_u)\n",
    "    Xte, yte, gte, mte = prep_for_lgb(cand_test_u)\n",
    "\n",
    "    ranker = lgb.LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"map\",\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=400,\n",
    "        num_leaves=45,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=2\n",
    "    )\n",
    "    ranker.fit(\n",
    "        Xtr, ytr, group=gtr,\n",
    "        eval_set=[(Xva, yva)],\n",
    "        eval_group=[gva],\n",
    "        eval_at=[1,3,5],\n",
    "        categorical_feature=cat_cols,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=150, verbose=False),\n",
    "            lgb.log_evaluation(50)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def rank_predict_ranker(model, X, meta):\n",
    "        s = model.predict(X, num_iteration=model.best_iteration_)\n",
    "        meta2 = meta.copy()\n",
    "        meta2[\"score\"] = s\n",
    "        preds, truth = [], []\n",
    "        for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "            g2 = g.sort_values(\"score\", ascending=False)\n",
    "            preds.append(g2[\"candidate\"].tolist())\n",
    "            truth.append(g2[\"label\"].iloc[0])\n",
    "        return preds, truth\n",
    "\n",
    "    preds_val, truth_val = rank_predict_ranker(ranker, Xva, mva)\n",
    "    preds_te,  truth_te  = rank_predict_ranker(ranker, Xte, mte)\n",
    "\n",
    "    m_val  = eval_topk_mrr([p[:5] for p in preds_val], truth_val, ks=(1,3,5))\n",
    "    m_test = eval_topk_mrr([p[:5] for p in preds_te],  truth_te,  ks=(1,3,5))\n",
    "\n",
    "    print(\"VAL:\",  m_val)\n",
    "    print(\"TEST:\", m_test)\n",
    "\n",
    "    # Save model\n",
    "    outm = PROCESSED_DIR / \"model_taskA_lgbm_ranker.txt\"\n",
    "    ranker.booster_.save_model(outm)\n",
    "    with open(PROCESSED_DIR / \"metrics_gbdt.json\", \"w\") as f:\n",
    "        json.dump({\"val\": m_val, \"test\": m_test}, f, indent=2)\n",
    "    print(\"Saved model:\", outm)\n",
    "\n",
    "else:\n",
    "    def to_xy(df_pl: pl.DataFrame):\n",
    "        df2 = df_pl.select(keep_cols).unique(subset=[\"sample_port_call_id\",\"candidate\"])\n",
    "        pdf = df2.to_pandas()\n",
    "        X = pdf[num_cols + cat_cols]\n",
    "        y = pdf[\"y\"].astype(int).values\n",
    "        meta = pdf[[\"sample_port_call_id\",\"origin\",\"candidate\",\"label\"]]\n",
    "        return X, y, meta\n",
    "\n",
    "    Xtr, ytr, mtr = to_xy(cand_train_ds)\n",
    "    Xva, yva, mva = to_xy(cand_val_u)\n",
    "    Xte, yte, mte = to_xy(cand_test_u)\n",
    "\n",
    "    numeric = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "                        (\"std\", StandardScaler())])\n",
    "    preproc = ColumnTransformer(\n",
    "        transformers=[(\"num\", numeric, num_cols),\n",
    "                      (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols)],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    clf = LogisticRegression(max_iter=200, solver=\"saga\", n_jobs=-1, verbose=0)\n",
    "    pipe = Pipeline([(\"prep\", preproc), (\"clf\", clf)])\n",
    "    pipe.fit(Xtr, ytr)\n",
    "\n",
    "    def rank_predict_clf(pipe, X, meta):\n",
    "        if hasattr(pipe.named_steps['clf'], 'predict_proba'):\n",
    "            proba = pipe.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            proba = pipe.decision_function(X)\n",
    "        meta2 = meta.copy()\n",
    "        meta2[\"score\"] = proba\n",
    "        preds, truth = [], []\n",
    "        for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "            g2 = g.sort_values(\"score\", ascending=False)\n",
    "            preds.append(g2[\"candidate\"].tolist())\n",
    "            truth.append(g2[\"label\"].iloc[0])\n",
    "        return preds, truth\n",
    "\n",
    "    preds_val, truth_val = rank_predict_clf(pipe, Xva, mva)\n",
    "    preds_te,  truth_te  = rank_predict_clf(pipe, Xte, mte)\n",
    "\n",
    "    m_val  = eval_topk_mrr([p[:5] for p in preds_val], truth_val, ks=(1,3,5))\n",
    "    m_test = eval_topk_mrr([p[:5] for p in preds_te],  truth_te,  ks=(1,3,5))\n",
    "\n",
    "    print(\"VAL:\",  m_val)\n",
    "    print(\"TEST:\", m_test)\n",
    "\n",
    "    outm = PROCESSED_DIR / \"model_taskA_hgb_ranker.joblib\"\n",
    "    joblib.dump(pipe, outm)\n",
    "    with open(PROCESSED_DIR / \"metrics_gbdt.json\", \"w\") as f:\n",
    "        json.dump({\"val\": m_val, \"test\": m_test}, f, indent=2)\n",
    "    print(\"Saved model:\", outm)\n",
    "\n",
    "# Dump Top‑5\n",
    "def dump_topk(meta, preds, split_name, k=5):\n",
    "    rows = []\n",
    "    # meta can be pandas DataFrame from prep\n",
    "    sids = meta[\"sample_port_call_id\"].unique()\n",
    "    for sid, plist in zip(sids, preds):\n",
    "        rows.append({\"sample_port_call_id\": sid, \"pred_topk\": plist[:k]})\n",
    "    outp = PROCESSED_DIR / f\"rank_top{str(k)}_{split_name}.parquet\"\n",
    "    pl.DataFrame(rows).write_parquet(outp)\n",
    "    print(\"Saved:\", outp)\n",
    "\n",
    "dump_topk(mva, preds_val, \"val\", k=5)\n",
    "dump_topk(mte, preds_te,  \"test\", k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "298699b3-864e-46c0-aaad-1379e41ebab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column</th><th>n_unique</th><th>values</th></tr><tr><td>str</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;origin&quot;</td><td>1980</td><td>[&quot;AMPCO&quot;, &quot;ASENG FPSO&quot;, … &quot;Zueitina&quot;]</td></tr><tr><td>&quot;candidate&quot;</td><td>2038</td><td>[&quot;AMPCO&quot;, &quot;ASENG FPSO&quot;, … &quot;Zueitina&quot;]</td></tr><tr><td>&quot;vessel_type&quot;</td><td>13</td><td>[&quot;Asphalt/Bitumen Tanker&quot;, &quot;Bulk Carrier&quot;, … &quot;Products Tanker &quot;]</td></tr><tr><td>&quot;dwt_bucket&quot;</td><td>5</td><td>[&quot;10–50k&quot;, &quot;120–200k&quot;, … &quot;&lt;10k&quot;]</td></tr><tr><td>&quot;product_family_dom&quot;</td><td>12</td><td>[&quot;ammonia&quot;, &quot;chem/bio&quot;, … &quot;olefins&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 3)\n",
       "┌────────────────────┬──────────┬─────────────────────────────────┐\n",
       "│ column             ┆ n_unique ┆ values                          │\n",
       "│ ---                ┆ ---      ┆ ---                             │\n",
       "│ str                ┆ i64      ┆ list[str]                       │\n",
       "╞════════════════════╪══════════╪═════════════════════════════════╡\n",
       "│ origin             ┆ 1980     ┆ [\"AMPCO\", \"ASENG FPSO\", … \"Zue… │\n",
       "│ candidate          ┆ 2038     ┆ [\"AMPCO\", \"ASENG FPSO\", … \"Zue… │\n",
       "│ vessel_type        ┆ 13       ┆ [\"Asphalt/Bitumen Tanker\", \"Bu… │\n",
       "│ dwt_bucket         ┆ 5        ┆ [\"10–50k\", \"120–200k\", … \"<10k… │\n",
       "│ product_family_dom ┆ 12       ┆ [\"ammonia\", \"chem/bio\", … \"ole… │\n",
       "└────────────────────┴──────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_cols = [\"origin\", \"candidate\", \"vessel_type\", \"dwt_bucket\", \"product_family_dom\"]\n",
    "\n",
    "summary = []\n",
    "for col in cat_cols:\n",
    "    # 去重+去空值得到类别列表\n",
    "    uniq = (\n",
    "        cand_train_u.select(pl.col(col))\n",
    "          .drop_nulls()\n",
    "          .unique()\n",
    "          .sort(col)\n",
    "          .get_column(col)\n",
    "          .to_list()\n",
    "    )\n",
    "    summary.append({\n",
    "        \"column\": col,\n",
    "        \"n_unique\": len(uniq),\n",
    "        \"values\": uniq,            \n",
    "    })\n",
    "\n",
    "cat_summary = pl.DataFrame(summary)\n",
    "cat_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44131cc-9196-478f-a2b5-b9b8117bc3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct labels: 2019\n",
      "['AMPCO', 'ASENG FPSO', 'Aabenraa', 'Aalborg', 'Aamchit Port', 'Aarhus', 'Aasta Hansteen', 'Aberdeen', 'Aberdeen (WA)', 'Abidjan']\n"
     ]
    }
   ],
   "source": [
    "label_stats = (\n",
    "    cand_train_u\n",
    "    .select(pl.col(\"label\"))\n",
    "    .drop_nulls()\n",
    "    .unique()\n",
    "    .sort(\"label\")\n",
    ")\n",
    "\n",
    "n_labels = label_stats.height\n",
    "label_list = label_stats.get_column(\"label\").to_list()\n",
    "\n",
    "print(f\"Distinct labels: {n_labels}\")\n",
    "print(label_list[:10])                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpler-ds (Py3.11)",
   "language": "python",
   "name": "kpler-ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
