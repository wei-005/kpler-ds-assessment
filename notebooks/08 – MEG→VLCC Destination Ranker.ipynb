{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5671d3e",
   "metadata": {},
   "source": [
    "# 08 – MEG→VLCC Destination Ranker\n",
    "# ---------------------------------\n",
    "# Focused notebook to isolate Middle East Gulf (MEG) VLCC voyages and\n",
    "# train a LightGBM ranker with the same memory safeguards used in 04.\n",
    "\n",
    "## Prerequisites\n",
    "# - Run notebooks `03_candidates_logistic_ranker` & `04_candidates_gbdt_ranker`\n",
    "#   so cached candidate tables / enriched features already exist.\n",
    "# - Ensure raw CSVs are available locally (no iCloud placeholders).\n",
    "# - This notebook concentrates on crude-oil voyages departing MEG export ports\n",
    "#   on VLCC/Suezmax tankers; the smaller cohort both reduces memory pressure\n",
    "#   and gives a clearer signal to validate modelling ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0995d775-1b2c-4b15-8ac9-72e0a45674a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjustable: Add parent directory (which contains utils/) to Python search path\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  #  notebooks  sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb4202-5a94-4714-98d4-0d67a37d6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import lightgbm as lgb\n",
    "\n",
    "from utils.config import PROCESSED_DIR, INTERIM_DIR, DATA_DIR\n",
    "from utils.splits import temporal_split, add_crisis_flag\n",
    "from utils.candidates import build_origin_next_transitions, build_pc_coords\n",
    "from utils.features import (\n",
    "    build_ports_attr,\n",
    "    compute_port_degree,\n",
    "    attach_port_side,\n",
    "    build_sample_side,\n",
    "    merge_all_features,\n",
    ")\n",
    "from utils.metrics import eval_topk_mrr\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper utilities (same as 04)\n",
    "# ------------------------------------------------------------------\n",
    "def downsample_groupwise(\n",
    "    df: pl.DataFrame,\n",
    "    neg_per_pos: int = 8,\n",
    "    seed: int = 42,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Keep all positives; random-sample negatives per sample.\"\"\"\n",
    "    out = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    for _, sub in df.group_by(\"sample_port_call_id\", maintain_order=True):\n",
    "        pos = sub.filter(pl.col(\"y\") == 1)\n",
    "        neg = sub.filter(pl.col(\"y\") == 0)\n",
    "\n",
    "        if pos.height == 0:\n",
    "            k = min(10, neg.height)\n",
    "            if k > 0:\n",
    "                keep_idx = rng.choice(neg.height, size=k, replace=False)\n",
    "                out.append(neg[keep_idx])\n",
    "            continue\n",
    "\n",
    "        k = min(neg.height, neg_per_pos * pos.height)\n",
    "        if k > 0:\n",
    "            keep_idx = rng.choice(neg.height, size=k, replace=False)\n",
    "            out.append(pl.concat([pos, neg[keep_idx]]))\n",
    "        else:\n",
    "            out.append(pos)\n",
    "    return pl.concat(out) if out else df.head(0)\n",
    "\n",
    "\n",
    "def ensure_columns(df: pl.DataFrame, num_cols: list[str], cat_cols: list[str]) -> pl.DataFrame:\n",
    "    out = df\n",
    "    for c in num_cols:\n",
    "        if c not in out.columns:\n",
    "            out = out.with_columns(pl.lit(0.0).alias(c))\n",
    "    for c in cat_cols:\n",
    "        if c not in out.columns:\n",
    "            out = out.with_columns(pl.lit(\"unk\").alias(c))\n",
    "    return out\n",
    "\n",
    "\n",
    "def prep_for_lgb(\n",
    "    df_pl: pl.DataFrame,\n",
    "    keep_cols: list[str],\n",
    "    num_cols: list[str],\n",
    "    cat_cols: list[str],\n",
    "):\n",
    "    df_pl = ensure_columns(df_pl, num_cols, cat_cols)\n",
    "    df_pd = df_pl.select(keep_cols).to_pandas()\n",
    "\n",
    "    for c in cat_cols:\n",
    "        if c in df_pd.columns:\n",
    "            df_pd[c] = df_pd[c].astype(\"category\")\n",
    "    for c in num_cols:\n",
    "        if c in df_pd.columns:\n",
    "            df_pd[c] = (\n",
    "                pd.to_numeric(df_pd[c], errors=\"coerce\")\n",
    "                  .replace([np.inf, -np.inf], np.nan)\n",
    "                  .fillna(0.0)\n",
    "            )\n",
    "\n",
    "    X      = df_pd[num_cols + cat_cols]\n",
    "    y      = df_pd[\"y\"].astype(int).values\n",
    "    groups = df_pd.groupby(\"sample_port_call_id\")[\"candidate\"].size().tolist()\n",
    "    meta   = df_pd[[\"sample_port_call_id\", \"origin\", \"candidate\", \"label\"]]\n",
    "    return X, y, groups, meta\n",
    "\n",
    "\n",
    "def subsample(df: pl.DataFrame, n: int = 75_000, seed: int = 42) -> pl.DataFrame:\n",
    "    return df if df.height <= n else df.sample(n=n, seed=seed)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load base data and identify MEG VLCC cohort\n",
    "# ------------------------------------------------------------------\n",
    "samples  = pl.read_parquet(PROCESSED_DIR / \"samples_taskA.parquet\")\n",
    "pc_clean = pl.read_parquet(INTERIM_DIR / \"port_calls.cleaned.parquet\")\n",
    "vessels  = pl.read_csv(DATA_DIR / \"vessels.csv\")\n",
    "\n",
    "train_samples, val_samples, test_samples = temporal_split(samples)\n",
    "\n",
    "sample_side_all = build_sample_side(samples, pc_clean, vessels).with_columns(\n",
    "    pl.col(\"vessel_type\").fill_null(\"\").str.to_uppercase(),\n",
    "    pl.col(\"product_family_dom\").fill_null(\"\"),\n",
    ")\n",
    "\n",
    "# MEG export ports (UN/port names can扩展)\n",
    "MEG_PORTS = [\n",
    "    \"RAS TANURA\",\n",
    "    \"JUAYMAH TERMINAL\",\n",
    "    \"YABLU ISLAND\",\n",
    "    \"YANBU SOUTH TERMINAL\",\n",
    "    \"JUBAIL COMMERCIAL PORT\",\n",
    "    \"RAS AL KHAIR\",\n",
    "    \"AL JUBAIL\",\n",
    "    \"RAS LAFFAN\",\n",
    "    \"ABU DHABI\",\n",
    "    \"DAS ISLAND\",\n",
    "    \"FUJAIRAH\",\n",
    "    \"MINA AL AHMADI\",\n",
    "    \"KHALIFA BIN SALMAN\",\n",
    "    \"KHARG ISLAND\",\n",
    "]\n",
    "\n",
    "# Identify sample IDs where:\n",
    "#  - destination (current call) 属于 MEG_PORTS (作为 origin)\n",
    "#  - vessel type contains VLCC/SUEZ/CRUDE\n",
    "#  - product family crude oil/condensate (可选)\n",
    "meg_vlcc_ids = (\n",
    "    sample_side_all\n",
    "    .join(samples.select(\"sample_port_call_id\", \"destination\"), on=\"sample_port_call_id\", how=\"left\")\n",
    "    .filter(\n",
    "        pl.col(\"destination\").str.contains(\"|\".join(MEG_PORTS), literal=False)\n",
    "        & pl.col(\"vessel_type\").str.contains(\"VLCC|SUEZ|CRUDE\")\n",
    "    )\n",
    "    .select(\"sample_port_call_id\")\n",
    "    .unique()\n",
    ")\n",
    "\n",
    "meg_id_list = meg_vlcc_ids.get_column(\"sample_port_call_id\").to_list()\n",
    "\n",
    "def split_card(name: str, df: pl.DataFrame):\n",
    "    subset = df.filter(pl.col(\"sample_port_call_id\").is_in(meg_id_list))\n",
    "    share  = subset.height / df.height if df.height else 0.0\n",
    "    print(f\"{name:5s}: total={df.height:,}  MEG-VLCC={subset.height:,} ({share:.2%})\")\n",
    "\n",
    "print(\"MEG VLCC sample availability:\")\n",
    "split_card(\"train\", train_samples)\n",
    "split_card(\"val\",   val_samples)\n",
    "split_card(\"test\",  test_samples)\n",
    "\n",
    "meg_train_samples = train_samples.filter(pl.col(\"sample_port_call_id\").is_in(meg_id_list))\n",
    "meg_val_samples   = val_samples.filter(pl.col(\"sample_port_call_id\").is_in(meg_id_list))\n",
    "meg_test_samples  = test_samples.filter(pl.col(\"sample_port_call_id\").is_in(meg_id_list))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load cached candidates, filter, enrich, apply memory guards\n",
    "# ------------------------------------------------------------------\n",
    "cand_train = pl.read_parquet(PROCESSED_DIR / \"cand_train_cached.parquet\")\n",
    "cand_val   = pl.read_parquet(PROCESSED_DIR / \"cand_val_cached.parquet\")\n",
    "cand_test  = pl.read_parquet(PROCESSED_DIR / \"cand_test_cached.parquet\")\n",
    "\n",
    "cand_train = cand_train.filter(pl.col(\"sample_port_call_id\").is_in(meg_id_list))\n",
    "cand_val   = cand_val.filter(pl.col(\"sample_port_call_id\").is_in(meg_id_list))\n",
    "cand_test  = cand_test.filter(pl.col(\"sample_port_call_id\").is_in(meg_id_list))\n",
    "\n",
    "print(\"Raw MEG VLCC candidate rows:\", cand_train.height, cand_val.height, cand_test.height)\n",
    "\n",
    "pc_coords   = build_pc_coords(pc_clean)\n",
    "ports_attr  = build_ports_attr(pc_coords)\n",
    "transitions = build_origin_next_transitions(train_samples)  # 全量训练统计\n",
    "port_degree = compute_port_degree(transitions)\n",
    "\n",
    "sample_side_meg = sample_side_all.filter(pl.col(\"sample_port_call_id\").is_in(meg_id_list))\n",
    "\n",
    "lagged_splits = {\n",
    "    \"train\": add_crisis_flag(meg_train_samples),\n",
    "    \"val\":   add_crisis_flag(meg_val_samples),\n",
    "    \"test\":  add_crisis_flag(meg_test_samples),\n",
    "}\n",
    "\n",
    "def enrich_split(cand_df: pl.DataFrame, split: str) -> pl.DataFrame:\n",
    "    if cand_df.is_empty():\n",
    "        return pl.DataFrame()\n",
    "\n",
    "    split_df = lagged_splits[split]\n",
    "    if split_df.is_empty():\n",
    "        return pl.DataFrame()\n",
    "\n",
    "    ids = split_df.select(\"sample_port_call_id\").unique()\n",
    "    filtered = cand_df.join(ids, on=\"sample_port_call_id\", how=\"inner\")\n",
    "    if filtered.is_empty():\n",
    "        return pl.DataFrame()\n",
    "\n",
    "    enriched = attach_port_side(filtered, ports_attr, port_degree)\n",
    "    s_side_split = sample_side_meg.join(ids, on=\"sample_port_call_id\", how=\"inner\")\n",
    "    enriched = merge_all_features(enriched, s_side_split, split_df)\n",
    "\n",
    "    return (\n",
    "        enriched\n",
    "        .select([c for c in enriched.columns if not c.endswith(\"_port\")])\n",
    "        .unique(subset=[\"sample_port_call_id\", \"candidate\"])\n",
    "    )\n",
    "\n",
    "cand_train_meg = enrich_split(cand_train, \"train\")\n",
    "cand_val_meg   = enrich_split(cand_val,   \"val\")\n",
    "cand_test_meg  = enrich_split(cand_test,  \"test\")\n",
    "\n",
    "# Memory guards: tighter caps (cohort更小)\n",
    "cand_val_meg  = subsample(cand_val_meg,  n=50_000)\n",
    "cand_test_meg = subsample(cand_test_meg, n=50_000)\n",
    "\n",
    "if cand_train_meg.height > 400_000:\n",
    "    cand_train_meg = cand_train_meg.sample(n=400_000, seed=42)\n",
    "\n",
    "cand_train_ds = downsample_groupwise(cand_train_meg, neg_per_pos=6, seed=42)\n",
    "\n",
    "print(\"Enriched MEG VLCC rows:\")\n",
    "print(\"  train:\", cand_train_meg.height, \"-> downsampled:\", cand_train_ds.height)\n",
    "print(\"  val  :\", cand_val_meg.height)\n",
    "print(\"  test :\", cand_test_meg.height)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# LightGBM ranker (lighter config)\n",
    "# ------------------------------------------------------------------\n",
    "num_cols = [\n",
    "    \"dist_km\", \"is_same_region\", \"in_cnt\", \"out_cnt\", \"age\",\n",
    "    \"prev_dist_km\", \"last_leg_knots_est\",\n",
    "    \"month_sin\", \"month_cos\", \"dow_sin\", \"dow_cos\",\n",
    "    \"is_crisis_time\", \"dist_x_crisis\",\n",
    "]\n",
    "extra_num = [c for c in [\n",
    "    \"prior_prob_oc\", \"hist_cnt_oc\", \"prior_prob_oc_vtype\",\n",
    "    \"prior_prob_oc_dwt\", \"prior_prob_oc_laden\", \"prior_prob_oc_pf\",\n",
    "    \"geo_rank_in_sample\", \"log_dist_km\", \"cand_is_waypoint\",\n",
    "    \"in_cnt_cand\", \"out_cnt_cand\",\n",
    "] if c in cand_train_meg.columns]\n",
    "num_cols = list(dict.fromkeys(num_cols + extra_num))\n",
    "\n",
    "cat_cols = [\n",
    "    c for c in [\"origin\", \"candidate\", \"vessel_type\", \"dwt_bucket\", \"product_family_dom\"]\n",
    "    if c in cand_train_meg.columns\n",
    "]\n",
    "\n",
    "keep_cols = list(dict.fromkeys(\n",
    "    [\"sample_port_call_id\", \"origin\", \"candidate\", \"label\", \"y\", *num_cols, *cat_cols]\n",
    "))\n",
    "\n",
    "if cand_train_ds.is_empty() or cand_val_meg.is_empty():\n",
    "    print(\"⚠️  MEG cohort too small; skipping LightGBM.\")\n",
    "else:\n",
    "    Xtr, ytr, gtr, mtr = prep_for_lgb(cand_train_ds, keep_cols, num_cols, cat_cols)\n",
    "    Xva, yva, gva, mva = prep_for_lgb(cand_val_meg, keep_cols, num_cols, cat_cols)\n",
    "    Xte, yte, gte, mte = (\n",
    "        prep_for_lgb(cand_test_meg, keep_cols, num_cols, cat_cols)\n",
    "        if not cand_test_meg.is_empty() else (None, None, None, None)\n",
    "    )\n",
    "\n",
    "    ranker = lgb.LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"map\",\n",
    "        learning_rate=0.05,\n",
    "        n_estimators=250,\n",
    "        num_leaves=31,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    ranker.fit(\n",
    "        Xtr, ytr, group=gtr,\n",
    "        eval_set=[(Xva, yva)],\n",
    "        eval_group=[gva],\n",
    "        eval_at=[1, 3, 5],\n",
    "        categorical_feature=cat_cols,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=60, verbose=False),\n",
    "            lgb.log_evaluation(25),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    def rank_predict(model, X, meta):\n",
    "        scores = model.predict(X, num_iteration=model.best_iteration_)\n",
    "        meta2 = meta.copy()\n",
    "        meta2[\"score\"] = scores\n",
    "        preds, truth = [], []\n",
    "        for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "            g2 = g.sort_values(\"score\", ascending=False)\n",
    "            preds.append(g2[\"candidate\"].tolist())\n",
    "            truth.append(g2[\"label\"].iloc[0])\n",
    "        return preds, truth\n",
    "\n",
    "    preds_val, truth_val = rank_predict(ranker, Xva, mva)\n",
    "    val_metrics = eval_topk_mrr([p[:5] for p in preds_val], truth_val, ks=(1, 3, 5))\n",
    "    print(\"Validation metrics:\", val_metrics)\n",
    "\n",
    "    if Xte is not None:\n",
    "        preds_te, truth_te = rank_predict(ranker, Xte, mte)\n",
    "        test_metrics = eval_topk_mrr([p[:5] for p in preds_te], truth_te, ks=(1, 3, 5))\n",
    "        print(\"Test metrics:\", test_metrics)\n",
    "    else:\n",
    "        test_metrics = None\n",
    "        print(\"Test split empty for MEG cohort.\")\n",
    "\n",
    "    model_path   = PROCESSED_DIR / \"model_taskA_lgbm_ranker_meg_vlcc.txt\"\n",
    "    metrics_path = PROCESSED_DIR / \"metrics_gbdt_meg_vlcc.json\"\n",
    "    ranker.booster_.save_model(model_path)\n",
    "    with open(metrics_path, \"w\") as fh:\n",
    "        json.dump({\"val\": val_metrics, \"test\": test_metrics}, fh, indent=2)\n",
    "    print(\"Saved model:\", model_path)\n",
    "    print(\"Saved metrics:\", metrics_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpler-ds (Py3.11)",
   "language": "python",
   "name": "kpler-ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
