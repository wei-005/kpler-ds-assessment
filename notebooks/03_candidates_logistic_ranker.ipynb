{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c933a7",
   "metadata": {},
   "source": [
    "# 03  Candidate Recall + Logistic Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465f62a8-87c2-4d23-a2e7-0ea0b2ea5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjustable: Add parent directory (which contains utils/) to Python search path\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  #  notebooks  sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a062d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unified imports ===\n",
    "import polars as pl\n",
    "\n",
    "# --- Standard imports ---\n",
    "import numpy as np, pandas as pd, json, joblib\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from utils.config import DATA_DIR, INTERIM_DIR, PROCESSED_DIR\n",
    "from utils.etl_clean import ensure_interim\n",
    "from utils.splits import temporal_split, add_crisis_flag\n",
    "from utils.candidates import build_origin_next_transitions, global_mf_next, build_pc_coords, build_candidates_for_split\n",
    "from utils.features import build_ports_attr, compute_port_degree, attach_port_side, build_sample_side, merge_all_features\n",
    "from utils.metrics import eval_topk_mrr\n",
    "\n",
    "# Force Polars to use all CPU cores\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = str(os.cpu_count())\n",
    "\n",
    "# Display and string cache setup\n",
    "pl.Config.set_tbl_rows(5)\n",
    "pl.Config.set_tbl_cols(10)\n",
    "pl.Config.set_tbl_formatting(\"ASCII_FULL\")\n",
    "\n",
    "# Force Polars to use all CPU cores\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = str(os.cpu_count())\n",
    "\n",
    "# Compatibility: enable global string cache for joins\n",
    "if hasattr(pl, \"enable_string_cache\"):\n",
    "    pl.enable_string_cache()   # Polars ≥ 1.0\n",
    "elif hasattr(pl, \"toggle_string_cache\"):\n",
    "    pl.toggle_string_cache(True)  # legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87d61ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling training set (4,857,768 rows) down to 500k rows...\n",
      " Saved sampled training set to: /Users/wangwei/Documents/Folders/工作/Kpler/data/processed/cand_train_cached_500k.parquet\n",
      "cand_train: (1418642, 42)\n",
      "CPU times: user 2min 34s, sys: 6.04 s, total: 2min 41s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Load samples and cleaned table\n",
    "samples = pl.read_parquet(PROCESSED_DIR / \"samples_taskA.parquet\")\n",
    "pc = pl.read_parquet(INTERIM_DIR / \"port_calls.cleaned.parquet\")\n",
    "tr = pl.read_csv(DATA_DIR / \"trades.csv\",  try_parse_dates=True)\n",
    "vs = pl.read_csv(DATA_DIR / \"vessels.csv\", try_parse_dates=True)\n",
    "\n",
    "train, val, test = temporal_split(samples)\n",
    "train = add_crisis_flag(train); val = add_crisis_flag(val); test = add_crisis_flag(test)\n",
    "\n",
    "# Build transition tables and coordinate maps from training data\n",
    "trans = build_origin_next_transitions(train)\n",
    "g_top = global_mf_next(trans)\n",
    "pc_coords = build_pc_coords(pc)\n",
    "\n",
    "cache_train_path = PROCESSED_DIR / \"cand_train_cached.parquet\"\n",
    "cache_train_sampled_path = PROCESSED_DIR / \"cand_train_cached_500k.parquet\"\n",
    "\n",
    "if cache_train_sampled_path.exists():\n",
    "    cand_train = pl.read_parquet(cache_train_sampled_path)\n",
    "else:\n",
    "    if cache_train_path.exists():\n",
    "        cand_train = pl.read_parquet(cache_train_path)  \n",
    "    else:\n",
    "        cand_train = build_candidates_for_split(train, trans, pc_coords, add_true_label=True,  N=10, M=10, global_top1=g_top)\n",
    "        cand_train.write_parquet(cache_train_path)\n",
    "        \n",
    "    print(f\"Sampling training set ({len(cand_train):,} rows) down to 500k rows...\")\n",
    "    cand_train = cand_train.sample(n=min(500_000, len(cand_train)), seed=42)\n",
    "\n",
    "    cand_train.write_parquet(cache_train_sampled_path)\n",
    "    print(f\" Saved sampled training set to: {cache_train_sampled_path}\")\n",
    "\n",
    "cache_val_path = PROCESSED_DIR / \"cand_val_cached.parquet\"\n",
    "if cache_val_path.exists():\n",
    "    cand_val = pl.read_parquet(cache_val_path)\n",
    "else:\n",
    "    cand_val = build_candidates_for_split(val,  trans, pc_coords, add_true_label=True,  N=10, M=10, global_top1=g_top)\n",
    "    cand_val.write_parquet(cache_val_path)\n",
    "\n",
    "cache_test_path = PROCESSED_DIR / \"cand_test_cached.parquet\"\n",
    "if cache_test_path.exists():\n",
    "    cand_test = pl.read_parquet(cache_test_path)\n",
    "else:\n",
    "    cand_test = build_candidates_for_split(test, trans, pc_coords, add_true_label=False, N=10, M=10, global_top1=g_top)\n",
    "    cand_test.write_parquet(cache_test_path)\n",
    "\n",
    "ports_attr  = build_ports_attr(pc_coords)\n",
    "port_degree = compute_port_degree(trans)\n",
    "cand_train  = attach_port_side(cand_train, ports_attr, port_degree)\n",
    "cand_val    = attach_port_side(cand_val,   ports_attr, port_degree)\n",
    "cand_test   = attach_port_side(cand_test,  ports_attr, port_degree)\n",
    "\n",
    "s_side   = build_sample_side(samples, pc, vs)\n",
    "cand_train = merge_all_features(cand_train, s_side, train)\n",
    "cand_val   = merge_all_features(cand_val,   s_side, val)\n",
    "cand_test  = merge_all_features(cand_test,  s_side, test)\n",
    "\n",
    "print(\"cand_train:\", cand_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7b4e04-4dff-4dd5-b66b-ed94343cd9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_no_dup_names(df):\n",
    "    cols = df.columns\n",
    "    assert len(cols) == len(set(cols)), \\\n",
    "        f\"duplicate columns: {[c for c in set(cols) if cols.count(c)>1]}\"\n",
    "assert_no_dup_names(cand_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc861107-16b3-40ca-b053-ca2a5de2f7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL: {'hits@1': 0.005480329531502998, 'hits@3': 0.016654507926905215, 'hits@5': 0.024643689614063807, 'mrr': 0.012072145809979687}\n",
      "TEST: {'hits@1': 0.006820396232543034, 'hits@3': 0.018048531526933604, 'hits@5': 0.025982461838259176, 'mrr': 0.013557277409177365}\n",
      "Model saved to: /Users/wangwei/Documents/Folders/工作/Kpler/data/processed/model_taskA_logreg.joblib\n",
      "CPU times: user 10.7 s, sys: 3.88 s, total: 14.6 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Logistic Ranker — One-Hot + Logistic Regression\n",
    "num_cols = [\n",
    "    \"dist_km\",\"is_same_region\",\"in_cnt\",\"out_cnt\",\"age\",\n",
    "    \"prev_dist_km\",\"last_leg_knots_est\",\n",
    "    \"month_sin\",\"month_cos\",\"dow_sin\",\"dow_cos\",\n",
    "    \"is_crisis_time\",\"dist_x_crisis\"\n",
    "]\n",
    "cat_cols = [\"origin\",\"candidate\",\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"]\n",
    "\n",
    "def to_xy(df: pl.DataFrame):\n",
    "    \"\"\"Prepare feature matrix X, label y, and metadata for ranking.\"\"\"\n",
    "\n",
    "    # 1) make 'keep' UNIQUE \n",
    "    base = [\"sample_port_call_id\",\"origin\",\"candidate\",\"label\",\"y\"]\n",
    "    keep = list(dict.fromkeys(base + num_cols + cat_cols))  # preserves order, drops dups\n",
    "\n",
    "    # 2) add missing columns with defaults\n",
    "    for c in keep:\n",
    "        if c not in df.columns:\n",
    "            df = df.with_columns((pl.lit(0.0) if c in num_cols else pl.lit(\"unk\")).alias(c))\n",
    "\n",
    "    # 3) select & ensure one row per (sample, candidate)\n",
    "    df = df.select(keep).unique(subset=[\"sample_port_call_id\",\"candidate\"], keep=\"first\")\n",
    "\n",
    "    # 4) to pandas for sklearn\n",
    "    pdf = df.to_pandas()\n",
    "    X = pdf[num_cols + cat_cols]\n",
    "    y = pdf[\"y\"].astype(int).values\n",
    "    meta = pdf[[\"sample_port_call_id\",\"origin\",\"candidate\",\"label\"]]\n",
    "    return X, y, meta\n",
    "\n",
    "# === build matrices ===\n",
    "Xtr, ytr, mtr = to_xy(cand_train)\n",
    "Xva, yva, mva = to_xy(cand_val)\n",
    "Xte, yte, mte = to_xy(cand_test)\n",
    "\n",
    "# === pipeline ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "     ('imputer', SimpleImputer(strategy='median')),\n",
    "     ('scaler', StandardScaler())\n",
    " ])\n",
    " \n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "    (\"num\", numeric_transformer, num_cols), \n",
    "     (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\", \n",
    "    sparse_threshold=1.0\n",
    "    )\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\", n_jobs=-1)\n",
    "pipe = Pipeline([(\"prep\", preproc), (\"clf\", clf)])\n",
    "pipe.fit(Xtr, ytr)\n",
    "\n",
    "# === ranking ===\n",
    "def rank_predict(pipe, X, meta, ks=(1,3,5)):\n",
    "    proba = pipe.predict_proba(X)[:, 1]\n",
    "    meta2 = meta.copy()\n",
    "    meta2[\"score\"] = proba\n",
    "    topk, truth = {}, []\n",
    "    for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "        g2 = g.sort_values(\"score\", ascending=False)\n",
    "        topk[sid] = g2[\"candidate\"].tolist()\n",
    "        truth.append(g[\"label\"].iloc[0])\n",
    "    preds = [topk[sid] for sid in meta2[\"sample_port_call_id\"].unique()]\n",
    "    return preds, truth\n",
    "\n",
    "preds_val, truth_val = rank_predict(pipe, Xva, mva)\n",
    "preds_te,  truth_te  = rank_predict(pipe, Xte, mte)\n",
    "\n",
    "print(\"VAL:\",  eval_topk_mrr([p[:5] for p in preds_val], truth_val, ks=(1,3,5)))\n",
    "print(\"TEST:\", eval_topk_mrr([p[:5] for p in preds_te],  truth_te,  ks=(1,3,5)))\n",
    "\n",
    "outm = PROCESSED_DIR / \"model_taskA_logreg.joblib\"\n",
    "joblib.dump(pipe, outm)\n",
    "print(\"Model saved to:\", outm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpler-ds (Py3.11)",
   "language": "python",
   "name": "kpler-ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
