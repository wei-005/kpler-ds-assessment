{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88e5afb",
   "metadata": {},
   "source": [
    "# 04  Candidate Recall + GBDT Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8608fe32-5f56-4a99-a185-f6022efde4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjustable: Add parent directory (which contains utils/) to Python search path\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))  #  notebooks  sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8038614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/wangwei/Documents/Folders/工作/Kpler\n"
     ]
    }
   ],
   "source": [
    "#Project root\n",
    "from pathlib import Path\n",
    "proj_root = Path.cwd()\n",
    "if (proj_root.name.lower() == \"notebooks\" or (proj_root/\"utils\").exists() is False) and (proj_root.parent/\"utils\").exists():\n",
    "    proj_root = proj_root.parent\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.append(str(proj_root))\n",
    "print(\"Project root:\", proj_root)\n",
    "\n",
    "#Unified imports\n",
    "import numpy as np, pandas as pd, polars as pl, json, joblib\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from utils.config import DATA_DIR, INTERIM_DIR, PROCESSED_DIR\n",
    "from utils.etl_clean import ensure_interim\n",
    "from utils.splits import temporal_split, add_crisis_flag\n",
    "from utils.candidates import build_origin_next_transitions, global_mf_next, build_pc_coords, build_candidates_for_split\n",
    "from utils.features import build_ports_attr, compute_port_degree, attach_port_side, build_sample_side, merge_all_features\n",
    "from utils.metrics import eval_topk_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5135281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pl.read_parquet(PROCESSED_DIR / \"samples_taskA.parquet\")\n",
    "pc = pl.read_parquet(INTERIM_DIR / \"port_calls.cleaned.parquet\")\n",
    "tr = pl.read_csv(DATA_DIR / \"trades.csv\",  try_parse_dates=True)\n",
    "vs = pl.read_csv(DATA_DIR / \"vessels.csv\", try_parse_dates=True)\n",
    "\n",
    "train, val, test = temporal_split(samples)\n",
    "train = add_crisis_flag(train); val = add_crisis_flag(val); test = add_crisis_flag(test)\n",
    "\n",
    "trans = build_origin_next_transitions(train)\n",
    "g_top = global_mf_next(trans)\n",
    "pc_coords = build_pc_coords(pc)\n",
    "\n",
    "cand_train = build_candidates_for_split(train, trans, pc_coords, add_true_label=True,  N=10, M=10, global_top1=g_top)\n",
    "cand_val   = build_candidates_for_split(val,   trans, pc_coords, add_true_label=True,  N=10, M=10, global_top1=g_top)\n",
    "cand_test  = build_candidates_for_split(test,  trans, pc_coords, add_true_label=False, N=10, M=10, global_top1=g_top)\n",
    "\n",
    "ports_attr  = build_ports_attr(pc_coords)\n",
    "port_degree = compute_port_degree(trans)\n",
    "cand_train  = attach_port_side(cand_train, ports_attr, port_degree)\n",
    "cand_val    = attach_port_side(cand_val,   ports_attr, port_degree)\n",
    "cand_test   = attach_port_side(cand_test,  ports_attr, port_degree)\n",
    "\n",
    "s_side   = build_sample_side(samples, pc, vs)\n",
    "cand_train = merge_all_features(cand_train, s_side, train)\n",
    "cand_val   = merge_all_features(cand_val,   s_side, val)\n",
    "cand_test  = merge_all_features(cand_test,  s_side, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c9adad",
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateError",
     "evalue": "the name 'origin' is duplicate\n\nIt's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     meta \u001b[38;5;241m=\u001b[39m pdf[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_port_call_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y, meta, enc\n\u001b[0;32m---> 23\u001b[0m Xtr, ytr, mtr, enc \u001b[38;5;241m=\u001b[39m \u001b[43mto_xy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#  val/test  encoder\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mto_xy_with_enc\u001b[39m(df: pl\u001b[38;5;241m.\u001b[39mDataFrame, enc):\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mto_xy\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cat_cols:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns: df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(c))\n\u001b[0;32m---> 13\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     14\u001b[0m X_num \u001b[38;5;241m=\u001b[39m pdf[num_cols]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#  denseHGB \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/kpler-ds/lib/python3.11/site-packages/polars/dataframe/frame.py:10007\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[1;32m   9923\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   9924\u001b[0m \u001b[38;5;124;03mSelect columns from this DataFrame.\u001b[39;00m\n\u001b[1;32m   9925\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10000\u001b[0m \u001b[38;5;124;03m└───────────┘\u001b[39;00m\n\u001b[1;32m  10001\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  10002\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpolars\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazyframe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopt_flags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m QueryOptFlags\n\u001b[1;32m  10004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m  10005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  10006\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m> 10007\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQueryOptFlags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_eager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  10008\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/kpler-ds/lib/python3.11/site-packages/polars/_utils/deprecation.py:97\u001b[0m, in \u001b[0;36mdeprecate_streaming_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min-memory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kpler-ds/lib/python3.11/site-packages/polars/lazyframe/opt_flags.py:330\u001b[0m, in \u001b[0;36mforward_old_opt_flags.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m         optflags \u001b[38;5;241m=\u001b[39m cb(optflags, kwargs\u001b[38;5;241m.\u001b[39mpop(key))  \u001b[38;5;66;03m# type: ignore[no-untyped-call,unused-ignore]\u001b[39;00m\n\u001b[1;32m    329\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizations\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m optflags\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kpler-ds/lib/python3.11/site-packages/polars/lazyframe/frame.py:2407\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, cluster_with_columns, collapse_joins, no_optimization, engine, background, optimizations, **_kwargs)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;66;03m# Only for testing purposes\u001b[39;00m\n\u001b[1;32m   2406\u001b[0m callback \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost_opt_callback\u001b[39m\u001b[38;5;124m\"\u001b[39m, callback)\n\u001b[0;32m-> 2407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mDuplicateError\u001b[0m: the name 'origin' is duplicate\n\nIt's possible that multiple expressions are returning the same default column name. If this is the case, try renaming the columns with `.alias(\"new_name\")` to avoid duplicate column names."
     ]
    }
   ],
   "source": [
    "#  GBDT HGB\n",
    "num_cols = [\"dist_km\",\"is_same_region\",\"in_cnt\",\"out_cnt\",\"age\",\n",
    "            \"prev_dist_km\",\"last_leg_knots_est\",\"month_sin\",\"month_cos\",\"dow_sin\",\"dow_cos\",\n",
    "            \"is_crisis_time\",\"dist_x_crisis\"]\n",
    "cat_cols = [\"origin\",\"candidate\",\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"]\n",
    "\n",
    "def to_xy(df: pl.DataFrame):\n",
    "    keep = [\"sample_port_call_id\",\"origin\",\"candidate\",\"label\",\"y\"] + num_cols + cat_cols\n",
    "    for c in num_cols:\n",
    "        if c not in df.columns: df = df.with_columns(pl.lit(0.0).alias(c))\n",
    "    for c in cat_cols:\n",
    "        if c not in df.columns: df = df.with_columns(pl.lit(\"unk\").alias(c))\n",
    "    pdf = df.select(keep).to_pandas()\n",
    "    X_num = pdf[num_cols].values\n",
    "    #  denseHGB \n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    X_cat = enc.fit_transform(pdf[cat_cols])\n",
    "    X = np.hstack([X_num, X_cat])\n",
    "    y = pdf[\"y\"].values\n",
    "    meta = pdf[[\"sample_port_call_id\",\"origin\",\"candidate\",\"label\"]]\n",
    "    return X, y, meta, enc\n",
    "\n",
    "Xtr, ytr, mtr, enc = to_xy(cand_train)\n",
    "#  val/test  encoder\n",
    "def to_xy_with_enc(df: pl.DataFrame, enc):\n",
    "    keep = [\"sample_port_call_id\",\"origin\",\"candidate\",\"label\",\"y\"] + num_cols + cat_cols\n",
    "    for c in num_cols:\n",
    "        if c not in df.columns: df = df.with_columns(pl.lit(0.0).alias(c))\n",
    "    for c in cat_cols:\n",
    "        if c not in df.columns: df = df.with_columns(pl.lit(\"unk\").alias(c))\n",
    "    pdf = df.select(keep).to_pandas()\n",
    "    X_num = pdf[num_cols].values\n",
    "    X_cat = enc.transform(pdf[cat_cols])\n",
    "    X = np.hstack([X_num, X_cat])\n",
    "    y = pdf[\"y\"].values\n",
    "    meta = pdf[[\"sample_port_call_id\",\"origin\",\"candidate\",\"label\"]]\n",
    "    return X, y, meta\n",
    "\n",
    "Xva, yva, mva = to_xy_with_enc(cand_val, enc)\n",
    "Xte, yte, mte = to_xy_with_enc(cand_test, enc)\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_depth=8, learning_rate=0.08, max_iter=300)\n",
    "clf.fit(Xtr, ytr)\n",
    "\n",
    "def rank_predict_hgb(clf, X, meta, ks=(1,3,5)):\n",
    "    proba = clf.predict_proba(X)[:,1]\n",
    "    meta2 = meta.copy()\n",
    "    meta2[\"score\"] = proba\n",
    "    topk = {}\n",
    "    for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "        g2 = g.sort_values(\"score\", ascending=False)\n",
    "        topk[sid] = g2[\"candidate\"].tolist()\n",
    "    truth, preds = [], []\n",
    "    for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "        truth.append(g[\"label\"].iloc[0])\n",
    "        preds.append(topk[sid])\n",
    "    return preds, truth\n",
    "\n",
    "from utils.metrics import eval_topk_mrr\n",
    "preds_val, truth_val = rank_predict_hgb(clf, Xva, mva)\n",
    "preds_te,  truth_te  = rank_predict_hgb(clf, Xte, mte)\n",
    "print(\"VAL:\", eval_topk_mrr([p[:5] for p in preds_val], truth_val, ks=(1,3,5)))\n",
    "print(\"TEST:\", eval_topk_mrr([p[:5] for p in preds_te],  truth_te,  ks=(1,3,5)))\n",
    "\n",
    "#  encoder\n",
    "import joblib\n",
    "joblib.dump({\"clf\":clf, \"enc\":enc, \"num_cols\":num_cols, \"cat_cols\":cat_cols}, PROCESSED_DIR / \"model_taskA_gbdt.joblib\")\n",
    "print(\"saved:\", PROCESSED_DIR / \"model_taskA_gbdt.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpler-ds (Py3.11)",
   "language": "python",
   "name": "kpler-ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
