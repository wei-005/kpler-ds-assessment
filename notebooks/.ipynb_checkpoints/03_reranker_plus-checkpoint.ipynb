{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407756cb",
   "metadata": {},
   "source": [
    "\n",
    "# 03 · Destination Reranker (Plus)\n",
    "**Goal:** stronger Task‑A baseline by adding (1) larger candidate recall, (2) historical priors, and (3) structured geo/channel features.  \n",
    "**Pipeline:** build candidates → enrich features → pointwise LR reranker → Top‑K metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc5a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Path bootstrap (so `utils/` is importable if running inside notebooks/) ===\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "proj_root = Path.cwd()\n",
    "if (proj_root.name.lower() == \"notebooks\" or not (proj_root/\"utils\").exists()) and (proj_root.parent/\"utils\").exists():\n",
    "    proj_root = proj_root.parent\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.append(str(proj_root))\n",
    "print(\"Project root:\", proj_root)\n",
    "\n",
    "# === Unified imports ===\n",
    "import polars as pl\n",
    "import numpy as np, pandas as pd, json, joblib, time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from utils.config import DATA_DIR, INTERIM_DIR, PROCESSED_DIR\n",
    "from utils.splits import temporal_split, add_crisis_flag\n",
    "from utils.candidates import build_origin_next_transitions, global_mf_next, build_pc_coords, build_candidates_for_split\n",
    "from utils.features import build_ports_attr, compute_port_degree, attach_port_side, build_sample_side, merge_all_features\n",
    "from utils.metrics import eval_topk_mrr\n",
    "from utils.etl_clean import ensure_interim\n",
    "\n",
    "# Use all CPU cores for Polars\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = str(os.cpu_count())\n",
    "\n",
    "# Display and string cache\n",
    "pl.Config.set_tbl_rows(5)\n",
    "pl.Config.set_tbl_cols(10)\n",
    "pl.Config.set_tbl_formatting(\"ASCII_FULL\")\n",
    "if hasattr(pl, \"enable_string_cache\"):\n",
    "    pl.enable_string_cache()\n",
    "elif hasattr(pl, \"toggle_string_cache\"):\n",
    "    pl.toggle_string_cache(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aebd3e7",
   "metadata": {},
   "source": [
    "### 1) Load samples & cleaned tables; temporal split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load / rebuild samples_taskA if missing\n",
    "samples_path = PROCESSED_DIR / \"samples_taskA.parquet\"\n",
    "if not samples_path.exists():\n",
    "    print(\"samples_taskA.parquet not found -> rebuilding from cleaned port_calls...\")\n",
    "    pc_clean = ensure_interim()  # returns cleaned port_calls as Polars DataFrame\n",
    "    pc_clean = pc_clean.sort([\"vessel_id\",\"start_utc\"])\n",
    "    pc_clean = pc_clean.with_columns([\n",
    "        pl.col(\"id\").shift(-1).over(\"vessel_id\").alias(\"next_call_id\"),\n",
    "        pl.col(\"destination\").shift(-1).over(\"vessel_id\").alias(\"next_call_name\"),\n",
    "        pl.col(\"start_utc\").alias(\"call_ts\")\n",
    "    ])\n",
    "    keep_cols = [\n",
    "        \"id\",\"vessel_id\",\"destination\",\"destination_latitude\",\"destination_longitude\",\n",
    "        \"call_ts\",\"next_call_id\",\"next_call_name\",\"is_load\",\"is_discharge\",\n",
    "        \"prev_dist_km\",\"last_leg_knots_est\",\"product_family_dom\"\n",
    "    ]\n",
    "    keep_cols = [c for c in keep_cols if c in pc_clean.columns]\n",
    "    samples = pc_clean.select(keep_cols).rename({\"id\":\"sample_port_call_id\"})\n",
    "    samples = samples.filter(pl.col(\"next_call_name\").is_not_null())\n",
    "    samples.write_parquet(samples_path)\n",
    "else:\n",
    "    samples = pl.read_parquet(samples_path)\n",
    "\n",
    "# Ensure call_ts is datetime\n",
    "if samples.schema.get(\"call_ts\") == pl.Utf8:\n",
    "    samples = samples.with_columns(pl.col(\"call_ts\").str.strptime(pl.Datetime, strict=False))\n",
    "\n",
    "# Load cleaned port_calls & static CSVs\n",
    "pc = pl.read_parquet(INTERIM_DIR / \"port_calls.cleaned.parquet\")\n",
    "tr = pl.read_csv(DATA_DIR / \"trades.csv\",  try_parse_dates=True)\n",
    "vs = pl.read_csv(DATA_DIR / \"vessels.csv\", try_parse_dates=True)\n",
    "\n",
    "# Temporal split (Jan-Sep Train, Oct-Nov Val, Dec Test inside utils.splits)\n",
    "train, val, test = temporal_split(samples)\n",
    "train = add_crisis_flag(train); val = add_crisis_flag(val); test = add_crisis_flag(test)\n",
    "\n",
    "print(\"train/val/test rows:\", train.height, val.height, test.height)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d50eff",
   "metadata": {},
   "source": [
    "### 2) Build transitions, coordinates, and candidate sets (larger N/M for better recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transitions (from training only) & global-most-frequent next\n",
    "trans = build_origin_next_transitions(train)\n",
    "g_top = global_mf_next(trans)\n",
    "\n",
    "# Coordinates\n",
    "pc_coords = build_pc_coords(pc)\n",
    "\n",
    "# Candidate sets (N/M enlarged to 30/30); keep cached files if present\n",
    "def maybe_build_or_load(path, builder):\n",
    "    if path.exists():\n",
    "        return pl.read_parquet(path)\n",
    "    df = builder()\n",
    "    df.write_parquet(path)\n",
    "    return df\n",
    "\n",
    "cand_train_path = PROCESSED_DIR / \"cand_train_N30_M30.parquet\"\n",
    "cand_val_path   = PROCESSED_DIR / \"cand_val_N30_M30.parquet\"\n",
    "cand_test_path  = PROCESSED_DIR / \"cand_test_N30_M30.parquet\"\n",
    "\n",
    "cand_train = maybe_build_or_load(\n",
    "    cand_train_path,\n",
    "    lambda: build_candidates_for_split(train, trans, pc_coords, add_true_label=True,  N=30, M=30, global_top1=g_top)\n",
    ")\n",
    "cand_val = maybe_build_or_load(\n",
    "    cand_val_path,\n",
    "    lambda: build_candidates_for_split(val,   trans, pc_coords, add_true_label=True,  N=30, M=30, global_top1=g_top)\n",
    ")\n",
    "cand_test = maybe_build_or_load(\n",
    "    cand_test_path,\n",
    "    lambda: build_candidates_for_split(test,  trans, pc_coords, add_true_label=False, N=30, M=30, global_top1=g_top)\n",
    ")\n",
    "\n",
    "print(\"cand_train / val / test shapes:\", cand_train.shape, cand_val.shape, cand_test.shape)\n",
    "\n",
    "# Optional: downsample training candidates for speed (keeps all positives)\n",
    "sampled_path = PROCESSED_DIR / \"cand_train_N30_M30_sampled500k.parquet\"\n",
    "if sampled_path.exists():\n",
    "    cand_train = pl.read_parquet(sampled_path)\n",
    "else:\n",
    "    pos = cand_train.filter(pl.col(\"y\")==1)\n",
    "    neg = cand_train.filter(pl.col(\"y\")==0)\n",
    "    target_n = 500_000 - pos.height\n",
    "    target_n = max(50_000, target_n)  # keep a floor\n",
    "    frac = min(1.0, target_n / max(1, neg.height))\n",
    "    neg = neg.sample(n=int(target_n), seed=42) if frac >= 1.0 else neg.sample(frac=frac, seed=42)\n",
    "    cand_train = pl.concat([pos, neg])\n",
    "    cand_train.write_parquet(sampled_path)\n",
    "print(\"cand_train after sampling:\", cand_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb0768",
   "metadata": {},
   "source": [
    "### 3) Attach port-side features and sample-side features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12969516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Port attributes and network degrees\n",
    "ports_attr  = build_ports_attr(pc_coords)\n",
    "port_degree = compute_port_degree(trans)\n",
    "\n",
    "cand_train  = attach_port_side(cand_train, ports_attr, port_degree)\n",
    "cand_val    = attach_port_side(cand_val,   ports_attr, port_degree)\n",
    "cand_test   = attach_port_side(cand_test,  ports_attr, port_degree)\n",
    "\n",
    "# Sample-side features (vessel static, speed/seasonal, laden/product flags)\n",
    "s_side = build_sample_side(samples, pc, vs)\n",
    "\n",
    "cand_train = merge_all_features(cand_train, s_side, train)\n",
    "cand_val   = merge_all_features(cand_val,   s_side, val)\n",
    "cand_test  = merge_all_features(cand_test,  s_side, test)\n",
    "\n",
    "print(\"cols:\", len(cand_train.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304c405",
   "metadata": {},
   "source": [
    "### 4) Add historical priors & structured geo/channel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- (A) Base prior: P(candidate | origin) and hist counts from training transitions ----\n",
    "prior_base = (\n",
    "    trans\n",
    "    .with_columns(pl.col(\"cnt\").sum().over(\"destination\").alias(\"tot\"))\n",
    "    .with_columns((pl.col(\"cnt\")/pl.col(\"tot\")).alias(\"prior_prob_oc\"))\n",
    "    .select([\"destination\",\"next_call_name\",\"prior_prob_oc\",\"cnt\"])\n",
    "    .rename({\"destination\":\"origin\",\"next_call_name\":\"candidate\",\"cnt\":\"hist_cnt_oc\"})\n",
    ")\n",
    "\n",
    "def join_priors(cdf: pl.DataFrame) -> pl.DataFrame:\n",
    "    return cdf.join(prior_base, on=[\"origin\",\"candidate\"], how=\"left\")\n",
    "\n",
    "cand_train = join_priors(cand_train)\n",
    "cand_val   = join_priors(cand_val)\n",
    "cand_test  = join_priors(cand_test)\n",
    "\n",
    "# ---- (B) Conditional priors: add vessel_type / dwt_bucket / laden / product_family_dom ----\n",
    "def add_conditional_prior(cdf: pl.DataFrame, keys: list[str], col_suffix: str) -> pl.DataFrame:\n",
    "    if not keys:\n",
    "        return cdf\n",
    "    # Use positive instances in training as observed transitions\n",
    "    src = cand_train.select([\"origin\",\"candidate\"] + [k for k in keys if k in cand_train.columns] + [\"y\"])\n",
    "    keys_present = [k for k in keys if k in src.columns]\n",
    "    if len(keys_present) != len(keys):\n",
    "        return cdf  # skip if some keys missing\n",
    "    src = (\n",
    "        src.filter(pl.col(\"y\")==1)\n",
    "           .group_by([\"origin\",\"candidate\"] + keys_present).len().rename({\"len\":\"cnt\"})\n",
    "           .with_columns(pl.col(\"cnt\").sum().over([\"origin\"] + keys_present).alias(\"tot\"))\n",
    "           .with_columns((pl.col(\"cnt\")/pl.col(\"tot\")).alias(f\"prior_prob_{col_suffix}\"))\n",
    "           .select([\"origin\",\"candidate\"] + keys_present + [f\"prior_prob_{col_suffix}\"])\n",
    "    )\n",
    "    return cdf.join(src, on=[\"origin\",\"candidate\"] + keys_present, how=\"left\")\n",
    "\n",
    "cand_train = add_conditional_prior(cand_train, [\"vessel_type\"],         \"oc_vtype\")\n",
    "cand_val   = add_conditional_prior(cand_val,   [\"vessel_type\"],         \"oc_vtype\")\n",
    "cand_test  = add_conditional_prior(cand_test,  [\"vessel_type\"],         \"oc_vtype\")\n",
    "\n",
    "cand_train = add_conditional_prior(cand_train, [\"dwt_bucket\"],          \"oc_dwt\")\n",
    "cand_val   = add_conditional_prior(cand_val,   [\"dwt_bucket\"],          \"oc_dwt\")\n",
    "cand_test  = add_conditional_prior(cand_test,  [\"dwt_bucket\"],          \"oc_dwt\")\n",
    "\n",
    "cand_train = add_conditional_prior(cand_train, [\"is_laden_after_call\"], \"oc_laden\")\n",
    "cand_val   = add_conditional_prior(cand_val,   [\"is_laden_after_call\"], \"oc_laden\")\n",
    "cand_test  = add_conditional_prior(cand_test,  [\"is_laden_after_call\"], \"oc_laden\")\n",
    "\n",
    "cand_train = add_conditional_prior(cand_train, [\"product_family_dom\"],  \"oc_pf\")\n",
    "cand_val   = add_conditional_prior(cand_val,   [\"product_family_dom\"],  \"oc_pf\")\n",
    "cand_test  = add_conditional_prior(cand_test,  [\"product_family_dom\"],  \"oc_pf\")\n",
    "\n",
    "# Fill NAs for prior columns\n",
    "def fill_prior_nas(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"prior_prob_\") or c in (\"hist_cnt_oc\",):\n",
    "            df = df.with_columns(pl.col(c).fill_null(0.0))\n",
    "    return df\n",
    "\n",
    "cand_train = fill_prior_nas(cand_train)\n",
    "cand_val   = fill_prior_nas(cand_val)\n",
    "cand_test  = fill_prior_nas(cand_test)\n",
    "\n",
    "# ---- (C) Structured geo/channel features ----\n",
    "WAYPOINT_RX = \"(?i)light|anchorage|canal|suez|panama|offshore|STS\"\n",
    "def add_geo_feats(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"candidate\").cast(pl.Utf8).str.contains(WAYPOINT_RX).fill_null(False).cast(pl.Int8).alias(\"cand_is_waypoint\"),\n",
    "        pl.when(pl.col(\"dist_km\").is_not_null()).then(pl.col(\"dist_km\").log1p()).otherwise(pl.lit(0.0)).alias(\"log_dist_km\"),\n",
    "    ])\n",
    "    # rank by distance within sample\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"dist_km\").is_not_null())\n",
    "          .then(pl.col(\"dist_km\").rank(\"min\").over(\"sample_port_call_id\"))\n",
    "          .otherwise(pl.lit(None))\n",
    "          .alias(\"geo_rank_in_sample\")\n",
    "    )\n",
    "    # fill missing ranks by large number (worse)\n",
    "    df = df.with_columns(pl.col(\"geo_rank_in_sample\").fill_null(pl.lit(9999)))\n",
    "    return df\n",
    "\n",
    "cand_train = add_geo_feats(cand_train)\n",
    "cand_val   = add_geo_feats(cand_val)\n",
    "cand_test  = add_geo_feats(cand_test)\n",
    "\n",
    "print(\"Feature enrich done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b96e0d",
   "metadata": {},
   "source": [
    "### 5) Candidate recall sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def candidate_recall(cands: pl.DataFrame) -> float:\n",
    "    return (\n",
    "        cands.group_by(\"sample_port_call_id\")\n",
    "             .agg(pl.col(\"y\").max().alias(\"has_truth\"))\n",
    "             .select(pl.col(\"has_truth\").mean())\n",
    "             .item()\n",
    "    )\n",
    "\n",
    "print({\n",
    "    \"recall_train\": float(candidate_recall(cand_train)),\n",
    "    \"recall_val\":   float(candidate_recall(cand_val)),\n",
    "    \"recall_test\":  float(candidate_recall(cand_test)),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04828e81",
   "metadata": {},
   "source": [
    "### 6) Train reranker (LR + priors) and evaluate Top‑K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cols = [\n",
    "    \"dist_km\",\"log_dist_km\",\"geo_rank_in_sample\",\n",
    "    \"is_same_region\",\"in_cnt\",\"out_cnt\",\"age\",\n",
    "    \"prev_dist_km\",\"last_leg_knots_est\",\n",
    "    \"month_sin\",\"month_cos\",\"dow_sin\",\"dow_cos\",\n",
    "    \"is_crisis_time\",\"dist_x_crisis\",\n",
    "    \"cand_is_waypoint\",\n",
    "    \"prior_prob_oc\",\"hist_cnt_oc\",\n",
    "    \"prior_prob_oc_vtype\",\"prior_prob_oc_dwt\",\"prior_prob_oc_laden\",\"prior_prob_oc_pf\",\n",
    "]\n",
    "cat_cols = [\"origin\",\"candidate\",\"vessel_type\",\"dwt_bucket\",\"product_family_dom\"]\n",
    "\n",
    "def to_xy(df: pl.DataFrame):\n",
    "    base = [\"sample_port_call_id\",\"origin\",\"candidate\",\"label\",\"y\"]\n",
    "    keep = list(dict.fromkeys(base + num_cols + cat_cols))\n",
    "    # add missing columns\n",
    "    for c in keep:\n",
    "        if c not in df.columns:\n",
    "            df = df.with_columns((pl.lit(0.0) if c in num_cols else pl.lit(\"unk\")).alias(c))\n",
    "    # select and unique per (sample, candidate)\n",
    "    df = df.select(keep).unique(subset=[\"sample_port_call_id\",\"candidate\"], keep=\"first\")\n",
    "    pdf = df.to_pandas()\n",
    "    X = pdf[num_cols + cat_cols]\n",
    "    y = pdf[\"y\"].astype(int).values\n",
    "    meta = pdf[[\"sample_port_call_id\",\"origin\",\"candidate\",\"label\"]]\n",
    "    return X, y, meta\n",
    "\n",
    "Xtr, ytr, mtr = to_xy(cand_train)\n",
    "Xva, yva, mva = to_xy(cand_val)\n",
    "Xte, yte, mte = to_xy(cand_test)\n",
    "\n",
    "# Build pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler(with_mean=False))  # with_mean=False works nicely with sparse later\n",
    "])\n",
    "\n",
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "preproc = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", ohe, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=1.0\n",
    ")\n",
    "\n",
    "try:\n",
    "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1, solver=\"saga\", C=0.5)\n",
    "except TypeError:\n",
    "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([(\"prep\", preproc), (\"clf\", clf)])\n",
    "\n",
    "t0 = time.time()\n",
    "pipe.fit(Xtr, ytr)\n",
    "print(f\"Fit time: {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Ranking & metrics\n",
    "def rank_predict(pipe, X, meta, k=5):\n",
    "    proba = pipe.predict_proba(X)[:, 1]\n",
    "    meta2 = meta.copy()\n",
    "    meta2[\"score\"] = proba\n",
    "    topk = {}\n",
    "    truth = {}\n",
    "    for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "        g2 = g.sort_values(\"score\", ascending=False)\n",
    "        topk[sid] = g2[\"candidate\"].tolist()\n",
    "        truth[sid] = g[\"label\"].iloc[0]\n",
    "    sids = list(topk.keys())\n",
    "    preds = [topk[sid] for sid in sids]\n",
    "    truths = [truth[sid] for sid in sids]\n",
    "    return preds, truths\n",
    "\n",
    "preds_val, truth_val = rank_predict(pipe, Xva, mva, k=5)\n",
    "preds_te,  truth_te  = rank_predict(pipe, Xte, mte,  k=5)\n",
    "\n",
    "print(\"VAL:\",  eval_topk_mrr([p[:5] for p in preds_val], truth_val, ks=(1,3,5)))\n",
    "print(\"TEST:\", eval_topk_mrr([p[:5] for p in preds_te],  truth_te,  ks=(1,3,5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89a688",
   "metadata": {},
   "source": [
    "### 7) Persist model and optional Top‑5 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c964e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outm = PROCESSED_DIR / \"model_taskA_logreg_plus.joblib\"\n",
    "joblib.dump(pipe, outm)\n",
    "print(\"Model saved to:\", outm)\n",
    "\n",
    "# Optional: persist top-5 candidate tables for inspection\n",
    "def dump_topk(meta: pd.DataFrame, scores: np.ndarray, out_path: Path, k=5):\n",
    "    meta2 = meta.copy()\n",
    "    meta2[\"score\"] = scores\n",
    "    rows = []\n",
    "    for sid, g in meta2.groupby(\"sample_port_call_id\"):\n",
    "        g2 = g.sort_values(\"score\", ascending=False).head(k).reset_index(drop=True)\n",
    "        for i, row in g2.iterrows():\n",
    "            rows.append({\n",
    "                \"sample_port_call_id\": sid,\n",
    "                \"rank\": i+1,\n",
    "                \"candidate\": row[\"candidate\"],\n",
    "                \"score\": row[\"score\"],\n",
    "                \"label\": row[\"label\"],\n",
    "            })\n",
    "    out_df = pl.from_pandas(pd.DataFrame(rows))\n",
    "    out_df.write_parquet(out_path)\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "# Save val/test top5\n",
    "from pathlib import Path as _Path\n",
    "val_scores = pipe.predict_proba(Xva)[:,1]\n",
    "test_scores= pipe.predict_proba(Xte)[:,1]\n",
    "dump_topk(mva, val_scores,  PROCESSED_DIR / \"val_top5_taskA_logreg_plus.parquet\",  k=5)\n",
    "dump_topk(mte, test_scores, PROCESSED_DIR / \"test_top5_taskA_logreg_plus.parquet\", k=5)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
