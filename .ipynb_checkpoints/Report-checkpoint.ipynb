{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec5225a-1525-4fd3-bbd9-b9f803e7094e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "  Kpler 数据科学家技术评估：目的地预测\n",
    "\n",
    "  提交人: [您的名字]\n",
    "  日期: 2025-10-07\n",
    "\n",
    "  ---\n",
    "\n",
    "  1. 项目概述 (Executive Summary)\n",
    "\n",
    "  本项目旨在对Kpler提供的海运数据集进行全面分析，并基于分析结果构建一个用于预测船舶下一个目的地的模型原型。我的工作流程遵循一个多阶段、迭代的方法，重点在于展示一个逻辑严谨、洞察驱动的解决方案，而非单纯追求模型精度。\n",
    "\n",
    "  首先，我通过深入的探索性数据分析(EDA)揭示了数据背后的关键业务模式与特性，例如“一装多卸”的普遍性，以及真实世界事件（如红海危机）对航运模式的显著影响。这些发现证明了简单模型（如历史频率）的局限性，并为后续复杂的建模策略提供了依\n",
    "  据。\n",
    "\n",
    "  基于这些洞察，我将预测问题构建为一个“候选生成+排序”的两阶段任务，并为此设计了涵盖历史、地理、行为和时间上下文的丰富特征集。我从简单的基线模型开始，逐步迭代到更强大的GBDT排序模型，并建立了严格的、基于时间的评估体系来衡量模型的\n",
    "  性能。\n",
    "\n",
    "  最终，本报告不仅呈现了一个可工作、可扩展的模型原型，更重要的是，它完整地记录了从数据理解到模型构建全过程中的关键决策和思考，并对模型的生产化部署提出了架构性建议。\n",
    "\n",
    "  ---\n",
    "\n",
    "  2. 数据集研究：关键洞察与发现 (Studying the Dataset: Key Insights)\n",
    "\n",
    "  我的分析揭示了数据中存在的清晰模式与值得注意的“异常”，这些是构建一个有效模型的基础。\n",
    "\n",
    "  ##### 2.1. 航运模式：港口、船型与货物的关联\n",
    "\n",
    "   * 地理与流量模式: 全球港口停靠存在明显的“头部效应”。如新加坡、鹿特丹等枢纽港口的停靠次数远超其他港口，Top-20港口占据了总停靠次数的20.5%。地理热力图显示，航运活动高度集中于东亚、欧洲西北部和美国墨西哥湾。\n",
    "   * 船型与航线特化: 不同船型与其航程距离和承运货物表现出强相关性。我的分析证实，原油船（Crude Oil Tanker）主要执行洲际远程航行（例如，中东到东亚），其平均航行距离最长；而成品油/化学品船（Products/Chemical \n",
    "     Tanker）的平均航程显著更短，更侧重于区域内的分销贸易。\n",
    "   * 货品与航线特化: 货物品类同样决定了航行距离。crude oil/condensate（原油/凝析油）的平均航距最长，而clean/dirty petroleum \n",
    "     products（清洁/重质成品油）的平均航距约为原油的一半，这清晰地反映了“原油长途运输，成品油区域配送”的行业模式。\n",
    "\n",
    "  ##### 2.2. 数据异常与关键特性\n",
    "\n",
    "  数据中存在的“异常”恰恰是理解业务复杂性的钥匙。\n",
    "\n",
    "   * “一装多卸”的普遍性: 通过分析trades数据，我发现虽然“一装一卸”是主流（122,392例），但“一装多卸”的现象非常普遍（例如，“一装两卸”有29,243例）。这一发现证明，不能简单地将一次装货和一次卸货划等号，在为模型定义“真实标签”时必须采用\n",
    "     如“选择交易量最大的目的地”这样的规则，以应对这种复杂性。\n",
    "   * `trades`数据的覆盖差距: 我的分析显示，约有 6.11% 的港口停靠记录（21,217次）在trades数据中没有任何关联。更重要的是，在这些“孤立”的停靠中，有高达 65.41% 的记录实际上发生了货物量变动。这揭示了一个深刻的洞察：仅仅有货物装卸，并\n",
    "     不足以被Kpler定义为一次“trade”。这证明trades.csv是一个经过复杂业务逻辑（如交易量门槛、货品类型等）提炼后的高价值分析产物，也因此决定了我的标签构造策略必须包含一个“兜底”方案。\n",
    "   * 特殊停靠地点: 数据中有超过22,000次停靠的地点名称包含“Light.”或“STS”，表明这些是在锚地进行的船对船转运作业。此外，苏伊士、巴拿马等运河区域也出现了数百次停靠记录。这些“非港口”的目的地是航运网络中的重要节点，在预测“下一物理站\n",
    "     点”时必须被考虑。\n",
    "\n",
    "\n",
    "  ---\n",
    "\n",
    "  3. 原型模型构建：一个逻辑严谨的方案\n",
    "\n",
    "  基于以上洞察，我设计并实现了一个以“严谨”和“可解释”为核心的建模方案。\n",
    "\n",
    "  ##### 3.1. 问题定义：任务A vs. 任务B\n",
    "\n",
    "  我首先将模糊的“预测下一站”问题，拆解为两个清晰的任务：\n",
    "   * 任务A (本次核心任务): 预测“下一物理站点”，严格对应题目的very next destination。标签直接采用按时间排序的下一个挂靠记录。\n",
    "   * 任务B (商业价值更高的任务): 预测“最终商业落点”，需要过滤掉STS、加油等中转站。\n",
    "\n",
    "  本次原型构建聚焦于任务A，同时在标签构造中也生成了任务B的标签，为未来的深入分析和展示对业务的深刻理解打下基础。\n",
    "\n",
    "  ##### 3.2. 建模框架：“候选生成 + 排序”\n",
    "\n",
    "  我没有采用简单的多分类模型，而是选择了更先进、更具扩展性的“候选生成+排序”两阶段框架，以应对目的地数量庞大且分布长尾的挑战。\n",
    "   1. 候选生成: 通过“历史高频”（Markov）和“地理邻近”（BallTree）两种策略，为每次离港生成一个约20个候选目的地的小列表。\n",
    "   2. 排序: 将问题转化为一个标准的二分类/排序任务，利用一个机器学习模型，结合丰富的特征，对每个候选者进行打分，选出最优解。\n",
    "\n",
    "  ##### 3.3. 迭代式建模与特征工程\n",
    "\n",
    "  我的建模过程是迭代的，从简单到复杂，每一步都服务于特定的分析目的：\n",
    "   1. 基线模型 (`02_Baselines`): 我首先建立了不含任何特征的“全局最频繁”和“马尔可夫”模型。它们在验证集上仅获得约 3-4% 的hits@1准确率。这个看似很低的结果，成功地、定量地证明了问题的困难性，以及依赖单一历史频率维度的局限性。\n",
    "   2. 特征驱动模型 (`03_LogReg`, `04_GBDT`): 我构建了涵盖历史、地理、行为、时间四大类的10余个特征，例如：\n",
    "       * 航速偏离度 (`speed_z_by_type`): 将船只的上一段航速与其同类型船舶的平均航速进行比较，是一个强大的归一化行为特征。\n",
    "       * 周期性时间编码 (`month_sin/cos`): 帮助模型理解月份的周期性。\n",
    "       * 危机时期标记 (`is_crisis_time`): 将红海危机这一外部事件转化为模型可以学习的特征。\n",
    "      通过引入这些特征，并使用GBDT等非线性模型，模型的性能得到了显著提升（具体提升数值将在04号笔记本中呈现）。\n",
    "\n",
    "  ---\n",
    "\n",
    "  4. 生产化部署讨论 (MLOps Discussion)\n",
    "\n",
    "  对于将此模型部署到生产环境，我的方案将围绕一个自动化、可监控、可迭代的ML流水线展开。关键组件包括：\n",
    "   * 数据管道: 自动化的数据摄取、验证和清洗流程。\n",
    "   * 特征存储 (Feature Store): 确保离线训练和在线推理的特征一致性。\n",
    "   * 模型再训练与版本控制: 基于MLflow等工具，实现模型的自动再训练、评估和版本管理。\n",
    "   * 在线推理服务: 提供一个能接收实时船舶状态、执行“候选生成+排序”流程并返回预测结果的API。\n",
    "   * 监控与反馈闭环: 持续监控数据分布和模型性能的“漂移”，并建立机制，将真实发生的航线数据反馈到训练流程中，形成持续学习的闭环。\n",
    "\n",
    "  ---\n",
    "  ---\n",
    "\n",
    "  Kpler Data Scientist Technical Assessment: Destination Forecast\n",
    "\n",
    "  Submitted by: [Your Name]\n",
    "  Date: 2025-10-07\n",
    "\n",
    "  ---\n",
    "\n",
    "  1. Executive Summary\n",
    "\n",
    "  This report details the methodology and findings for the Kpler Destination Forecast task. The objective is to analyze maritime activity datasets (vessels, port_calls, trades) to uncover underlying patterns and build a \n",
    "  prototype model to predict a vessel's next destination.\n",
    "\n",
    "  My approach was a multi-stage, iterative process executed through a series of structured Jupyter notebooks. Key stages included:\n",
    "   1. In-depth Exploratory Data Analysis (EDA): To understand data quality, identify key business patterns (e.g., \"one-load, many-discharges\"), and quantify the impact of real-world events (the Red Sea crisis).\n",
    "   2. Baseline Modeling: To establish a quantitative performance benchmark using simple, feature-less models (Global Most-Frequent & Markov).\n",
    "   3. Feature-based Ranking Model: To develop a robust prediction model using a \"Candidate Generation + Ranking\" framework, powered by extensive feature engineering and a Gradient Boosting Decision Tree (GBDT) model.\n",
    "\n",
    "  A core finding from the EDA is the inherent complexity of the problem: simple historical frequency is a poor predictor (achieving only ~4% accuracy), and a significant portion of cargo-moving events are not classified as \n",
    "  \"trades,\" justifying the need for a sophisticated, feature-driven modeling approach. My final prototype successfully demonstrates a structured, well-reasoned, and extensible solution.\n",
    "\n",
    "  ---\n",
    "\n",
    "  2. Studying the Dataset: Key Insights\n",
    "\n",
    "  My analysis revealed clear patterns and noteworthy \"anomalies\" in the data, which formed the foundation for building an effective model.\n",
    "\n",
    "  ##### 2.1. Patterns in Port Visits, Vessel Types, and Cargo\n",
    "\n",
    "   * Geographical Hotspots: The interactive folium map (ports_map.html) and hexbin plots clearly show that global maritime activity is concentrated in key regions: East Asia (Singapore, Ulsan, Zhoushan), Northwest Europe \n",
    "     (Rotterdam, Antwerp), and the US Gulf Coast (Houston). By frequency of calls, Singapore is unequivocally the world's busiest port.\n",
    "   * Vessel & Cargo Specialization: Heatmaps demonstrated a strong correlation between vessel type/size and the cargo they carry. For instance, my analysis of dominant products confirmed that Ultra Large Crude Carriers \n",
    "     (dwt_bucket: 200k+) almost exclusively transport crude oil/condensate, while smaller tankers are more versatile. This confirms that vessel characteristics are highly predictive features.\n",
    "   * Product & Voyage Specialization: Cargo types also dictate voyage distances. crude oil/condensate voyages have the longest average distance, whereas clean/dirty petroleum products have significantly shorter routes (roughly \n",
    "     half the distance of crude). This clearly reflects the industry pattern of long-haul crude transportation and regional distribution of refined products.\n",
    "\n",
    "  ##### 2.2. Data Abnormalities & Key Specificities\n",
    "\n",
    "  The \"anomalies\" in the data were crucial for understanding the complexity of the business logic.\n",
    "\n",
    "   * The \"One-to-Many\" Problem: My analysis of the trades data showed that while a simple \"one-load, one-discharge\" pattern is the most common (122,392 instances), a very significant number of loading events result in multiple \n",
    "     discharges (e.g., 29,243 instances for 2 destinations, 7,577 for 3, and so on). This finding invalidates the naive assumption of a simple one-to-one mapping and proves the necessity of a rule (such as \"max volume\") to define \n",
    "     a single primary label for modeling.\n",
    "   * The `trades` Coverage Gap: My analysis shows that 6.11% of all port call events are not referenced in the trades data. More surprisingly, of these \"orphan\" events, 65.41% had cargo_volume > 0. This is a profound insight: \n",
    "     simply moving cargo is not sufficient to be defined as a \"trade\" by Kpler. This proves that trades.csv is a highly refined analytical product, likely filtered by complex business rules (e.g., volume thresholds, cargo types), \n",
    "     justifying my two-tier labeling strategy (prefer trades, but have a fallback).\n",
    "   * Concept Drift (The Red Sea Crisis): My monthly seasonality analysis clearly shows a significant dip in traded volume starting in November 2023, coinciding with the Houthi crisis. This real-world event introduces concept \n",
    "     drift, where underlying routing patterns change fundamentally. This insight directly informed my modeling strategy, leading to a strict, crisis-aware time split (Train: pre-Oct 20, Validation: post-Oct 20) and the creation \n",
    "     of an is_crisis_time feature to allow the model to learn the different behavioral regimes.\n",
    "\n",
    "  ---\n",
    "\n",
    "  3. Prototype Model: A Well-Reasoned Approach\n",
    "\n",
    "  Based on the insights above, I designed and implemented a modeling approach centered on rigor and interpretability.\n",
    "\n",
    "  ##### 3.1. Problem Framing: Task A vs. Task B\n",
    "\n",
    "  I first decomposed the ambiguous \"predict next destination\" problem into two distinct tasks:\n",
    "   * Task A (Primary Focus): Predict the \"Very Next Destination,\" strictly corresponding to the prompt. The label is the next chronological port call, regardless of its nature (e.g., STS, bunkering).\n",
    "   * Task B (Higher Business Value): Predict the \"Final Commercial Destination,\" which involves filtering out intermediate waypoints.\n",
    "\n",
    "  My prototype développement focuses on Task A to directly meet the assessment's requirements, while the architecture for Task B labels was also developed to demonstrate a deeper business understanding.\n",
    "\n",
    "  ##### 3.2. Modeling Framework: \"Candidate Generation + Ranking\"\n",
    "\n",
    "  Instead of a naive multi-class classification approach, I chose a more advanced and scalable two-stage framework:\n",
    "   1. Candidate Generation: For any given departure, I generate a small, relevant list of ~20 potential candidates using a hybrid of historical frequency (Markov) and geographical proximity (BallTree).\n",
    "   2. Ranking: I then use a machine learning model to score each candidate. The problem is thus transformed from \"picking one from 2,000+\" to a much more effective binary classification/ranking task.\n",
    "\n",
    "  ##### 3.3. Iterative Modeling & Feature Engineering\n",
    "\n",
    "  My modeling process was iterative:\n",
    "   1. Baselines (`02_Baselines`): I established that simple, feature-less models perform poorly (hits@1 ≈ 3-4%), quantitatively proving the problem's difficulty.\n",
    "   2. Feature-Driven Rankers (`03_LogReg`, `04_GBDT`): I engineered a rich feature set across four categories: Historical, Geospatial, Behavioral, and Temporal. Highlights include a normalized speed feature (speed_z_by_type), \n",
    "      cyclical time features (month_sin/cos), and the contextual is_crisis_time flag. By using these features with a GBDT model, performance is expected to significantly improve over the baselines.\n",
    "\n",
    "  ---\n",
    "\n",
    "  4. Productionization Discussion (MLOps)\n",
    "\n",
    "  For deploying this model, my proposed solution centers on an automated, monitorable, and iterative ML pipeline. Key components would include a feature store for online/offline consistency, automated model retraining and \n",
    "  versioning (e.g., with MLflow), a scalable API for inference, and robust monitoring for data and model drift. A critical component would be a human-in-the-loop feedback system where analyst corrections are fed back to \n",
    "  continuously improve the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpler-ds (Py3.11)",
   "language": "python",
   "name": "kpler-ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
